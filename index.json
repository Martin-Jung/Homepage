[{"authors":[],"categories":["python","gis"],"content":"QGIS remains my go-to default QGIS software and I have contributed some python plugins to it in the past. These include the LecoS plugin, which has been cited 22 times so far and still appears to be widely used by researchers and practitioner alike [I do remember helping out with a workshop in Nairobi quite a while back, where everybody appeared to know the plugin].\n  However because of other commitments (writing up my PhD thesis at the moment) I am unable to further develop this neat little piece of software. Furthermore I programmed LecoS mainly for my own use in the past, while these days I would probably use the excellent and comprehensive landscapemetrics R-package. Nevertheless LecoS might still be useful for those people without coding knowledge and unwilling to apply a separate piece of software (Fragstats).\nI can now announce that LecoS has finally been ported to QGIS 3. This can almost entirely be credited to Caio Hamamura, who led the code restructuring from QGIS2 to QGIS3. The new LecoS version 3.0 for QGIS3 should already be available from the QGIS plugin server. Thanks Caio!\n","date":1551220386,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551220386,"objectID":"3e0d2cb579442162f376980f6f73f176","permalink":"http://martin-jung.github.io/post/2019-lecos-update/","publishdate":"2019-02-26T22:33:06Z","relpermalink":"/post/2019-lecos-update/","section":"post","summary":"QGIS remains my go-to default QGIS software and I have contributed some python plugins to it in the past. These include the LecoS plugin, which has been cited 22 times so far and still appears to be widely used by researchers and practitioner alike [I do remember helping out with a workshop in Nairobi quite a while back, where everybody appeared to know the plugin].\n  However because of other commitments (writing up my PhD thesis at the moment) I am unable to further develop this neat little piece of software.","tags":["QGIS","LecoS","Landscape-Ecology"],"title":"LecoS ported to QGIS3","type":"post"},{"authors":[],"categories":["phd"],"content":"This is a rather short post informing anyone who might read this post and is likely a Sussex PhD student (and presumably found this blog via google, hello google bot :-) ), that I created an unofficial template for Sussex PhD students. I ensured that the template is formatted as required by the Sussex doctoral school, however the template is provided as is and I am not liable for any errors or damages caused by using this template (see license in repository). I am not connected to the University of Sussex Doctoral School.\nhttps://github.com/Martin-Jung/Sussex_PhDThesis\nHere is the blank pdf to see the output. Obviously format and structure can entirely be edited, but this obviously requires more in depth knowledge of TeX. I recommend using overleaf for writing. The template itself might also be good as base to build your own template in case you are a PhD student from another University.\n","date":1547679600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547679600,"objectID":"49e1da145a98727bcb439336648c13f6","permalink":"http://martin-jung.github.io/post/2019-latex-template/","publishdate":"2019-01-17T00:00:00+01:00","relpermalink":"/post/2019-latex-template/","section":"post","summary":"This is a rather short post informing anyone who might read this post and is likely a Sussex PhD student (and presumably found this blog via google, hello google bot :-) ), that I created an unofficial template for Sussex PhD students. I ensured that the template is formatted as required by the Sussex doctoral school, however the template is provided as is and I am not liable for any errors or damages caused by using this template (see license in repository).","tags":["latex"],"title":"LateX template for Sussex PhD theses","type":"post"},{"authors":null,"categories":["statistics"],"content":" Running machine learning algorithms on large amounts of data can take considerable time. There are multiple ways of speeding up your code. The most obvious is to properly parallize your code and/or - assuming R or python is used - replace certain functions that cause a bottleneck to faster languages such as C++ or Julia. What also works is to simply have more computational power. In a previous post I elaborated on how to make use of the google cloud processing platform. Rather than CPU power another, often overlooked way, is to rely a bit more on the graphical processing unit (GPU) of the computer. Obviously this can be done in the cloud as well.\nHere is some data that I briefly worked on during my PhD. Those are globally distributed crowd-sourced land cover training sites from Fritz et al. (2016). They contain information about land cover broadly following the LCCS global land cover legend. For all sites I extracted spectral data from the MODIS satellites as well some other environmental data such as precipitation and elevation range or slope.\nHere is how the data looks:\n# Load packages library(caret) # harmonized model library(xgboost) # Extreme Gradient Boosting library(doParallel)\t# parallel processing library(mlbench) # machine learning optimization routines library(tidyverse) # Used by caret library(lubridate) # For time series objects library(gbm)\t# GBM Models library(jsonlite) # For the training data library(ggplot2); library(scales) library(GGally) # For sophisticated plotting # Load only the high resolution data (tagged used high-res images) training_fritz \u0026lt;- readRDS(\u0026quot;TrainingFritzPrepared.rds\u0026quot;) %\u0026gt;% dplyr::filter(Resolution == 1) # Only high resolution # These are land cover categories considered cols = c(Mosaic..Cultivated.and.managed...Natural.vegetation= \u0026quot;brown\u0026quot;, Tree.cover = \u0026quot;darkgreen\u0026quot;, Shrub.cover = \u0026quot;lightgreen\u0026quot;, Herbaceous.vegetation...Grassland = \u0026quot;orange\u0026quot;, Urban = \u0026quot;black\u0026quot;, Cultivated = \u0026quot;red\u0026quot;, Barren = \u0026quot;grey\u0026quot;, Water = \u0026quot;blue\u0026quot;) # Lets plot them gp \u0026lt;- ggpairs(data=training_fritz[,c(\u0026quot;Label\u0026quot;,\u0026quot;EVI2\u0026quot;,\u0026quot;NBR\u0026quot;,\u0026quot;NDMI\u0026quot;)], columns = c(\u0026quot;EVI2\u0026quot;,\u0026quot;NBR\u0026quot;,\u0026quot;NDMI\u0026quot;),#diag=\u0026quot;blank\u0026quot;, mapping=ggplot2::aes(colour = Label,alpha=.5)) # ------- # for (row in seq_len(gp$nrow)) for (col in seq_len(gp$ncol)) gp[row, col] \u0026lt;- gp[row, col] + ggplot2::scale_color_manual(values = cols) + scale_fill_manual(values = cols) gp   Paired correlation plot between all land cover classes and some vegetation indicators   Extreme Gradient descent boosting Gradient descent boosting methods commonly rule the leaderboard of most data science competitions with especially the Xgboost being one of the most applied algorithms for feature learning and prediction. Lets try to predict land cover using this dataset\n# First we create a training and out-of-bag testing dataset set.seed(31337) # Specfiy randomized seed # Create data partition samp \u0026lt;- createDataPartition(training_fritz$Label, p = 0.75, list = FALSE) # Split into training input and validate training \u0026lt;- training_fritz[samp,] # ~ 4568 rows test \u0026lt;- training_fritz[-samp,] # OOB observations # Repeated cross validation for control for caret ctrl \u0026lt;- trainControl( method=\u0026quot;repeatedcv\u0026quot;, # cross validation number=10, # 10-fold repeats = 5, # 5 times savePredictions=\u0026quot;final\u0026quot;, index=createResample(training$Label, 25), # The index for the ensemble classProbs=TRUE, allowParallel = TRUE, verboseIter = TRUE # Verbose output ) # Tuning parameters # Not going to specify anything here, but ideally one should have wide grid of parameters #tg.xgb \u0026lt;- expand.grid(nrounds = seq(125,200,by=25), lambda = c(0,0.01,0.001,0.0001,0.00001), alpha = c(0,0.01,0.001,0.0001,0.00001),eta= 0.3) # Now train x1 \u0026lt;- Sys.time() mod \u0026lt;- caret::train(as.formula( \u0026quot;Label ~ red + green + blue + nir + lst + swir1 + swir2 + elevation + slope + precip_range + NDVI + NDMI\u0026quot;), data = training, method = \u0026quot;xgbTree\u0026quot;, # xgb with Tree baselearner metric = \u0026quot;Accuracy\u0026quot;, trControl = ctrl, na.action = na.pass ) Sys.time() - x1 predict   CPU cores on my laptop heavily at work\u0026hellip;   The whole process takes on my relatively fast (ot at least it fast a few years ago) Thinkpad P50 laptop roughly 14.27 minutes. The highest accuracy on the training data alone of this model was 0.7646, while the accuracy on the withold OOB data was 0.7831 ( Kappa : 0.7208 ), which is fair.\n Now lets execute the same model but with enabled CUDA support. Note that in order to do this you need to have a GPU that support CUDA. My Laptop has a Quadro 1000M GPU, which is certainly not the fastest, but should do the trick. Furthermore xgboost has not by default GPU enabled so you need to recompile it before doing this. Here is how I did this on my Linux machine Remember to deinstall the xgboost package first\ngit clone --recursive https://github.com/dmlc/xgboost cd xgboost mkdir build cd build cmake .. -DUSE_CUDA=ON -DR_LIB=ON make install -j  If everything worked fine you should have a new package with GPU support. I had to relink my gcc++ libraries and getting the correct cuda version installed took some time. Now lets rerun:\n# For GPU processing reset the parallel processer as I have a single GPU ctrl \u0026lt;- trainControl( method=\u0026quot;repeatedcv\u0026quot;, # cross validation number=10, # 10-fold repeats = 5, # 5 times savePredictions=\u0026quot;final\u0026quot;, index=createResample(training$Label, 25), # The index for the ensemble classProbs=TRUE, allowParallel = F, verboseIter = TRUE # Verbose output ) x1 \u0026lt;- Sys.time() mod_gpu \u0026lt;- caret::train(as.formula( \u0026quot;Label ~ red + green + blue + nir + lst + swir1 + swir2 + elevation + slope + precip_range + NDVI + NDMI\u0026quot;), data = training, method = \u0026quot;xgbTree\u0026quot;, # xgb with Tree baselearner metric = \u0026quot;Accuracy\u0026quot;, updater = 'grow_gpu', tree_method = \u0026quot;gpu_exact\u0026quot;, trControl = ctrl, na.action = na.pass ) Sys.time() - x1  Total execution speed now is down to 12.42 minutes! Quite decent for my old GPU, but I guess a more powerful CPU might get even more out of it.\nSome inspiration for this blogpost came from this PeerJ manuscript:\n Mitchell R, Frank E. (2017) Accelerating the XGBoost algorithm using GPU computing. PeerJ Computer Science 3:e127 https://doi.org/10.7717/peerj-cs.127  ","date":1542322800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542322800,"objectID":"51a54464b85beaba16c3a7c880679b14","permalink":"http://martin-jung.github.io/post/2018_xgbboostcuda/","publishdate":"2018-11-16T00:00:00+01:00","relpermalink":"/post/2018_xgbboostcuda/","section":"post","summary":"Running machine learning algorithms on large amounts of data can take considerable time. There are multiple ways of speeding up your code. The most obvious is to properly parallize your code and/or - assuming R or python is used - replace certain functions that cause a bottleneck to faster languages such as C++ or Julia. What also works is to simply have more computational power. In a previous post I elaborated on how to make use of the google cloud processing platform.","tags":["rstats","boosting","gpu"],"title":"Using the GPU for gradient descent boosting","type":"post"},{"authors":[],"categories":["data-science","phd","statistics"],"content":"Why argue for Bayesian models? Most researchers and data scientists have specific - domain - knowledge about the subject they analyse data for. In a Bayesian analysis framework this knowledge can be refereed to as Prior and the effect und uncertainty surrounding this. Most standard analytical tools do not account for this information. In fact every statistical tool makes some kind of assumptions about your data. Computers do not now per se how your data looks or what limits it. By not providing this kind of information, the algorithm essentially has to guess or make uninformed assumptions, which can be quite unrealistic in applied settings.\nOr as Richard McElreath puts it: Still seeing researchers rejecting Bayes bc priors make them uncomfortable.\nIf you don\u0026#39;t like priors, then don\u0026#39;t use Bayes. But you must do something to control overfitting. And that something will act a lot like a prior. https://t.co/FAZdcDS4B7\n\u0026mdash; Richard McElreath (@rlmcelreath) August 22, 2018  Here is an example using data from my PhD. In many of my PhD chapters I work with the PREDICTS database, a global database of local biodiversity records. A particular feature of the PREDICTS data collection is that the sites of biodiversity sampling are \u0026ldquo;re-categorized\u0026rdquo; into land-use categories such as Primary vegetation or Pasture.\nA number of high-profile papers have been produced using PREDICTS data such as the one by my old supervisor, who used Hierarchical linear mixed effects models to estimate the average difference in species richness with land use globally (Newbold et al. 2015). Have a look at Newbold et al. for more information about the study design and methods.\nThe interesting question here is: Should we consider land use as a monotonic gradient? Or in other words: Do we expect that the difference between all categories other than the intercept (Primary vegetation) is negative, so a loss in species richness? Another information that we can include is the expected difference or slope of local species loss. For instance we also can\u0026rsquo;t have more than 100 % loss of species richness for any site, so we have thus information about the - prior - distribution of the expected slope\nMost PREDICTS data has been released and is openly available through the National History Museum London and it should be possible to prepare a data frame similar to the one I used here.\n\nLets first fit a roughly comparable model to Newbold et al. 2015:\n# Load packages and data for this project library(tidyverse) library(tidybayes) library(brms) library(lme4) library(sjPlot) # Load biodiversity site dataset sites \u0026lt;- readRDS(\u0026quot;~/../PhD/Projects/P5_MagnitudeBreakpoints/PREDICTS_allsites.rds\u0026quot;) # Merge primary vegetation to single category as reference sites \u0026lt;- subset(sites,Predominant_habitat != \u0026quot;Cannot decide\u0026quot;)# Drop Cannot decide from the factors sites$Predominant_habitat \u0026lt;- fct_collapse(sites$Predominant_habitat,\u0026quot;Primary vegetation\u0026quot; = c(\u0026quot;Primary forest\u0026quot;,\u0026quot;Primary non-forest\u0026quot;)) # Here all the categories we consider sites$Predominant_habitat \u0026lt;- factor(sites$Predominant_habitat, levels = c(\u0026quot;Primary vegetation\u0026quot;,\u0026quot;Young secondary vegetation\u0026quot;,\u0026quot;Intermediate secondary vegetation\u0026quot;,\u0026quot;Mature secondary vegetation\u0026quot;, \u0026quot;Secondary vegetation (indeterminate age)\u0026quot;,\u0026quot;Plantation forest\u0026quot;,\u0026quot;Pasture\u0026quot;,\u0026quot;Cropland\u0026quot;,\u0026quot;Urban\u0026quot;), ordered = T) sites$SS \u0026lt;- factor(sites$SS) # A prediction container for later nd \u0026lt;- data.frame(Predominant_habitat = factor(levels(sites$Predominant_habitat),levels = levels(sites$Predominant_habitat),ordered = T) ) # Prediction frame # A function to transform all estimates relative to the intercept (that is primary vegetation) relIntercept \u0026lt;- function(df){ return( df %\u0026gt;% group_by(model) %\u0026gt;% mutate(fit = ( fit / fit[which(Predominant_habitat==\u0026quot;Primary vegetation\u0026quot;)])-1, fit.low = (fit.low / fit[which(Predominant_habitat==\u0026quot;Primary vegetation\u0026quot;)])-1, fit.high = (fit.high / fit[which(Predominant_habitat==\u0026quot;Primary vegetation\u0026quot;)])-1 ) %\u0026gt;% ungroup() ) } # First for lme4 using a \u0026quot;classic\u0026quot; predicts model and reproduce a model similar to Newbold et al. 2015 # As simple test we on use two random intercepts, the study and a spatial block within study fit1 \u0026lt;- glmer(Species_richness ~ Predominant_habitat + (1|SS) + (1|SSB), data = sites,family = poisson(link = \u0026quot;log\u0026quot;)) # Save output out \u0026lt;- data.frame( Predominant_habitat = nd$Predominant_habitat, fit = predict(fit1,newdata=nd,re.form=NA,type = \u0026quot;link\u0026quot;), fit.se = arm::se.fixef(fit1), # Get the standard error of the coefficients model = \u0026quot;lme4\u0026quot; ) %\u0026gt;% mutate( fit.low = exp((fit - fit.se*1.96)), # naively multiply the wald standard errors with 1.96 fit.high = exp((fit + fit.se*1.96)), fit = exp(fit) )  Now lets create a similar model using a Bayesian framework. Here I will use the excellent brms package to skip the lengthy procedure of writing Stan code myself. This is a very simple model for demonstration purposes and can very likely quite improved.\nThe Equation for the bayesian multi-level model :\n$$ \\begin{eqnarray} \\text{Species richness}_i \u0026amp; \\sim \u0026amp; \\text{Poisson} (\\mu_i) \\\\\n\\text{log} (\\mu_i) \u0026amp; = \u0026amp; \\alpha + \\alpha_{\\text{Study}_i} + \\alpha_{\\text{Spatial block}_i} + \\beta (\\text{Land use}_i) \\\\\n\\beta \u0026amp; \\sim \u0026amp; \\text{Normal} (0, 1) \\\\\n\\alpha_{\\text{Study,Spatial block}} \u0026amp; \\sim \u0026amp; \\text{Normal} (0, \\sigma_{\\text{Study,Spatial block}}) \\\\\n\\sigma_{\\text{Study,Spatial block}} \u0026amp; \\sim \u0026amp; \\text{HalfCauchy} (0, 2) \\\\\n\\end{eqnarray} $$ Priors for both intercepts are identical although one could argue that the spatial block should be specified differently given that it is nested within study.\nand can be fitted like thos:\nfit2 \u0026lt;- brm(Species_richness ~ Predominant_habitat + (1|SS) + (1|SSB), data = sites,family = poisson(link = \u0026quot;log\u0026quot;), prior = prior(normal(0,1), class = b) + # Normal distributed prior for beta prior(cauchy(0,2), class = sd), # Half cauchy prior for uncertainty chains = 2, cores = 6, iter = 2000) # Fit using 6 cores and 2000 iterations of 2 MCMC chains # ... This takes a while. # Made myself dinner # How did we do? # (showing only the intercept) plot(fit2, ask =F)     There are some irregularities in the iterations of the MCMC that do not look completely random yet and more interations might help. Or alternatively specifying a prior for the intercept :)   \nLets compare the two models in their predicted impact of land use on species richness.\n# Get the prediction from the bayesian model out \u0026lt;- bind_rows(out, # Summarise from the fitted posterior values fitted_draws(fit2,nd,re_formula = NA) %\u0026gt;% group_by(Predominant_habitat) %\u0026gt;% mean_qi(.value) %\u0026gt;% select(Predominant_habitat,.value,.lower,.upper) %\u0026gt;% rename(fit = \u0026quot;.value\u0026quot;,fit.low = \u0026quot;.lower\u0026quot;, fit.high = \u0026quot;.upper\u0026quot;) %\u0026gt;% mutate(fit.se = NA, model = \u0026quot;brms1\u0026quot;) ) # Plot them both ggplot(out %\u0026gt;% relIntercept(.), aes(x = fct_rev( Predominant_habitat ), y = fit, ymin = fit.low, ymax = fit.high, group = model, color =model)) + theme_default() + coord_flip()+ geom_pointrange(position = position_dodge(.5)) + geom_hline(yintercept = 0, linetype = \u0026quot;dotted\u0026quot;)+ scale_y_continuous(breaks = scales::pretty_breaks(5)) + scale_colour_brewer(palette = \u0026quot;Set1\u0026quot;) + labs(x=\u0026quot;\u0026quot;,y = \u0026quot;Predicted difference in Species richness\u0026quot;) + theme(axis.text.x = element_text(size = 15))      Ignore the error bars for now as they are not directly comparable (see below).   It is noteworthy that the uncertainty for lme4 is the estimated standard error of the fitted object (!), while for brms the estimation error is drawn only from the fitted values of the posterior.\nStill so far both models are quite similar and indicate comparable, mean losses of species richness as shown in the Newbold et al. 2015 Nature paper. Now here comes the interesting part. Paul Brückner, the author of the brms R package has recently released a new preprint discussing the addition of monotonic effects to the brms package. Monotonic functions can be applied ordinal predictors for which a monotonic relationship (increasing, decreasing, concave, convex) is highly plausible.\nFitting those kind of models in brms is now straight forward. We use the default uniform Dirichlet priors which according to Brückner generalize well.\nfit3 \u0026lt;- brm(Species_richness ~ mo(Predominant_habitat) + (1|SS) + (1|SSB), data = sites,family = poisson(link = \u0026quot;log\u0026quot;), prior = prior(normal(0,1), class = b) + prior(cauchy(0,2), class = sd), chains = 2, cores = 6, iter = 2000) # Lets compare both models waic(fit2,fit3)  It does seem that the model without a monotonic effect is a better fit to the data.\nWAIC SE fit2 170240.73 990.35 fit3 170533.55 997.99 fit2 - fit3 -292.82 99.12  How do the fitted effects compare ? Quite differently. Notice how local species richness monotonically declines with every further \u0026ldquo;level\u0026rdquo; of land use.\n  Overall it does seem as if we have to reject the idea of \u0026ldquo;land-use gradients\u0026rdquo; at least if those gradients are quantified through categorical entities. One could argue that some - human altered - land-use categories can have higher number of species than some natural land use. Thinking for example of urban habitats for plants (lots of gardens, exotics and non-natives).\nIn my PhD I try to find ways to capture land dynamics on a continuous unit scale rather than through categorical estimates. Look our for new results of this idea soon\u0026hellip;\nSessioninfo\nR version 3.4.4 (2018-03-15) Platform: x86_64-pc-linux-gnu (64-bit) Running under: Ubuntu 18.04.1 LTS Matrix products: default BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1 LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1 locale: [1] LC_CTYPE=en_GB.UTF-8 LC_NUMERIC=C LC_TIME=en_GB.UTF-8 LC_COLLATE=en_GB.UTF-8 [5] LC_MONETARY=en_GB.UTF-8 LC_MESSAGES=en_GB.UTF-8 LC_PAPER=en_GB.UTF-8 LC_NAME=C [9] LC_ADDRESS=C LC_TELEPHONE=C LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C attached base packages: [1] parallel stats graphics grDevices utils datasets methods base other attached packages: [1] tidybayes_1.0.3 bindrcpp_0.2.2 sjPlot_2.6.1 mboost_2.9-1 stabs_0.6-3 lme4_1.1-19 Matrix_1.2-14 [8] brms_2.6.0 Rcpp_1.0.0 forcats_0.3.0 stringr_1.3.1 dplyr_0.7.8 purrr_0.2.5 readr_1.1.1 [15] tidyr_0.8.2 tibble_1.4.2 ggplot2_3.1.0 tidyverse_1.2.1 loaded via a namespace (and not attached): [1] readxl_1.1.0 backports_1.1.2 plyr_1.8.4 igraph_1.2.2 [5] lazyeval_0.2.1 svUnit_0.7-12 TMB_1.7.15 splines_3.4.4 [9] crosstalk_1.0.0 TH.data_1.0-9 rstantools_1.5.1 inline_0.3.15 [13] digest_0.6.18 htmltools_0.3.6 rsconnect_0.8.11 lmerTest_3.0-1 [17] fansi_0.4.0 magrittr_1.5 modelr_0.1.2 matrixStats_0.54.0 [21] xts_0.11-2 sandwich_2.5-0 prettyunits_1.0.2 colorspace_1.3-2 [25] rvest_0.3.2 haven_1.1.2 callr_3.0.0 crayon_1.3.4 [29] jsonlite_1.5 libcoin_1.0-1 bindr_0.1.1 survival_2.42-3 [33] zoo_1.8-4 glue_1.3.0 gtable_0.2.0 nnls_1.4 [37] emmeans_1.3.0 sjstats_0.17.2 sjmisc_2.7.6 pkgbuild_1.0.2 [41] rstan_2.18.2 abind_1.4-5 scales_1.0.0 mvtnorm_1.0-8 [45] ggeffects_0.6.0 miniUI_0.1.1.1 xtable_1.8-3 HDInterval_0.2.0 [49] ggstance_0.3.1 foreign_0.8-70 Formula_1.2-3 stats4_3.4.4 [53] prediction_0.3.6 StanHeaders_2.18.0 DT_0.5 htmlwidgets_1.3 [57] httr_1.3.1 threejs_0.3.1 arrayhelpers_1.0-20160527 RColorBrewer_1.1-2 [61] modeltools_0.2-22 pkgconfig_2.0.2 loo_2.0.0 utf8_1.1.4 [65] labeling_0.3 tidyselect_0.2.5 rlang_0.3.0.1 reshape2_1.4.3 [69] later_0.7.5 munsell_0.5.0 cellranger_1.1.0 tools_3.4.4 [73] cli_1.0.1 sjlabelled_1.0.14 broom_0.5.0 ggridges_0.5.1 [77] arm_1.10-1 yaml_2.2.0 processx_3.2.0 knitr_1.20 [81] coin_1.2-2 nlme_3.1-137 mime_0.6 xml2_1.2.0 [85] debugme_1.1.0 compiler_3.4.4 bayesplot_1.6.0 shinythemes_1.1.2 [89] rstudioapi_0.8 stringi_1.2.4 ps_1.2.1 Brobdingnag_1.2-6 [93] lattice_0.20-35 psych_1.8.10 nloptr_1.2.1 markdown_0.8 [97] shinyjs_1.0 stringdist_0.9.5.1 pillar_1.3.0 pwr_1.2-2 [101] bridgesampling_0.6-0 estimability_1.3 data.table_1.11.8 httpuv_1.4.5 [105] R6_2.3.0 promises_1.0.1 gridExtra_2.3 codetools_0.2-15 [109] colourpicker_1.0 MASS_7.3-50 gtools_3.8.1 assertthat_0.2.0 [113] withr_2.1.2 shinystan_2.5.0 mnormt_1.5-5 multcomp_1.4-8 [117] hms_0.4.2 quadprog_1.5-5 grid_3.4.4 rpart_4.1-13 [121] coda_0.19-2 glmmTMB_0.2.2.0 minqa_1.2.4 inum_1.0-0 [125] snakecase_0.9.2 partykit_1.2-2 numDeriv_2016.8-1 shiny_1.2.0 [129] lubridate_1.7.4 base64enc_0.1-3 dygraphs_1.1.1.6  ","date":1541372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541372400,"objectID":"94022cbbfed9e0c4f72ade7fa9609579","permalink":"http://martin-jung.github.io/post/2018_bayesianmodelsmonoticpredictors/","publishdate":"2018-11-05T00:00:00+01:00","relpermalink":"/post/2018_bayesianmodelsmonoticpredictors/","section":"post","summary":"Why argue for Bayesian models? Most researchers and data scientists have specific - domain - knowledge about the subject they analyse data for. In a Bayesian analysis framework this knowledge can be refereed to as Prior and the effect und uncertainty surrounding this. Most standard analytical tools do not account for this information. In fact every statistical tool makes some kind of assumptions about your data. Computers do not now per se how your data looks or what limits it.","tags":["rstats","bayesian statistics","PREDICTS"],"title":"Testing multilevel Bayesian models with ordered categorical predictors","type":"post"},{"authors":["Martin Jung","Pedram Rowhani","Tim Newbold","Laura Bentley","Andy Purvis","Jörn PW Scharlemann"],"categories":null,"content":"","date":1535925600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535925600,"objectID":"b9b5d3c0b71647d07a3b51b0e5afedd1","permalink":"http://martin-jung.github.io/publication/2018_jungpairwisedifferences/","publishdate":"2018-09-03T00:00:00+02:00","relpermalink":"/publication/2018_jungpairwisedifferences/","section":"publication","summary":"Most land on Earth has been changed by humans and past changes of land can have lasting influences on current species assemblages. Yet few globally representative studies explicitly consider such influences even though auxiliary data, such as from remote sensing, are readily available. Time series of satellite-derived data have been commonly used to quantify differences in land-surface attributes such as vegetation cover, which will among other things be influenced by anthropogenic land conversions and modifications. Here we quantify differences in current and past (up to five years before sampling) vegetation cover, and assess whether such differences differentially influence taxonomic and functional groups of species assemblages between spatial pairs of sites. Specifically, we correlated between-site dissimilarity in photosynthetic activity of vegetation (the Enhanced Vegetation Index) with the corresponding dissimilarity in local species assemblage composition from a global database using a common metric for both, the Bray-Curtis index. We found that dissimilarity in species assemblage composition was on average more influenced by dissimilarity in past than current photosynthetic activity, and that the influence of past dissimilarity increased when longer time periods were considered. Responses to past dissimilarity in photosynthetic activity also differed among taxonomic groups (plants, invertebrates, amphibians, reptiles, birds and mammals), with reptiles being among the most influenced by more dissimilar past photosynthetic activity. Furthermore, we found that assemblages dominated by smaller and more vegetation-dependent species tended to be more influenced by dissimilarity in past photosynthetic activity than prey-dependent species. Overall, our results have implications for studies that investigate species responses to current environmental changes and highlight the importance of past changes continuing to influence local species assemblage composition. We demonstrate how local species assemblages and satellite-derived data can be linked and provide suggestions for future studies on how to assess the influence of past environmental changes on biodiversity.","tags":["biodiversity","remote-sensing","MODIS","PREDICTS"],"title":"Local species assemblages are influenced more by past than current dissimilarities in photosynthetic activity","type":"publication"},{"authors":[""],"categories":["phd","remote-sensing","PREDICTS","biodiversity"],"content":"I can now happily announce that the first chapter of my PhD has been accepted and is going to be published in the Journal Ecography. In this piece I investigated whether pairwise compositional differences between species assemblages (so in the identity and amount of species found in a given place and time) can be explained by a dissimilarity in remotely-sensed vegetation index. I furthermore found that past dissimilarities in photosynthetic activity improved the overall fit relative to current dissimilarities, therefore hinting at a lag-effect that can be investigated via remote sensing. Also quite cool is that we use - to my knowledge for the first time - the same dissimilarity metric (the Bray-Curtis index) to quantify both differences between species assemblages and entire remotely-sensed time series of photosynthetic activity. Terminology and writeup took quite a while for this piece, so I am quite happy that it is finally out. You can find the abstract and links to paper etc. here.\n","date":1535925600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535925600,"objectID":"aa62c808ae98bc7c964ea3c01715f7b4","permalink":"http://martin-jung.github.io/post/2018_newphdpublication/","publishdate":"2018-09-03T00:00:00+02:00","relpermalink":"/post/2018_newphdpublication/","section":"post","summary":"I can now happily announce that the first chapter of my PhD has been accepted and is going to be published in the Journal Ecography. In this piece I investigated whether pairwise compositional differences between species assemblages (so in the identity and amount of species found in a given place and time) can be explained by a dissimilarity in remotely-sensed vegetation index. I furthermore found that past dissimilarities in photosynthetic activity improved the overall fit relative to current dissimilarities, therefore hinting at a lag-effect that can be investigated via remote sensing.","tags":["PhD","remote-sensing","biodiversity"],"title":"New PhD paper - Pairwise differences in photosynthetic activity","type":"post"},{"authors":null,"categories":["data-science","cloud-computing"],"content":" In this new post I will go through my process of getting familiar with running R in the Google cloud and the posting sort of follows my previous post on getting started with the Google cloud. My dream setup would include to being able to switch seamless between running r code locally or in the cloud whenever I require more processing power. For instance similar doAzureParallel package available for Microsoft Azure. For Google cloud engine, there also exists a neat package called googleComputeEngineR, that allows to easily setup a virtual machine and run code remotely. So let\u0026rsquo;s setup the googleComputeEngineR package. As always, please note you alone (the dear reader) is responsible to keep track of your virtual machines in the cloud. If you do not stop them (i.e. shut them down), then this will cost you money!  In order to use the googleComputeEngineR package, we first need to create a credentials file. For my google cloud project and personal linux machine I have created such a file on my local system like this:\n# Create the file touch ~/.Renviron echo \u0026quot;GCE_AUTH_FILE=\\\u0026quot;~/wasserdampf.json\\\u0026quot;\u0026quot; \u0026gt;\u0026gt; ~/.Renviron echo \u0026quot;GCE_DEFAULT_PROJECT_ID=\\\u0026quot;wolke7-208420\\\u0026quot;\u0026quot; \u0026gt;\u0026gt; ~/.Renviron echo \u0026quot;GCE_DEFAULT_ZONE=\\\u0026quot;us-central1-a\\\u0026quot;\u0026quot; \u0026gt;\u0026gt; ~/.Renviron  One also needs a service account auth key (here called wasserdampf.json). Find more information how to get such a key here. Now for starters lets start R and install the googleComputeEngineR package, then start up a virtual machine with Rstudio setup.\nRun a Rstudio in the google cloud llibrary(googleAuthR) library(googleComputeEngineR) library(future) # Start up a rstudio vm (or create if not already existing) vm \u0026lt;- gce_vm(template = \u0026quot;rstudio\u0026quot;, name = \u0026quot;rstudio\u0026quot;, username = \u0026quot;martin\u0026quot;, password = \u0026quot;wolkenwind\u0026quot;, predefined_type = \u0026quot;n1-standard-1\u0026quot; # Available machines via gce_list_machinetype() ) # See if the vm exists gce_list_instances()  \u0026gt; ==Google Compute Engine Instance List== \u0026gt; name machineType status zone externalIP creationTimestamp \u0026gt; 1 rstudio n1-standard-1 RUNNING us-central1-a XX.XXX.XXX.XXX 2018-08-23 14:41:45  The externalIP gives the ip through which rstudio server can be run in any webbrowser Equally it is quite easy to control the VM via SSH directly in the browser and the googleComputeEngineR package provides an easy function to open such a connection:\ngce_ssh_browser(vm)  Lastly ensure that you stop the VM(or delete it).\n# Shut down the vm gce_vm_stop(vm) # Or delete the vm gce_vm_delete(vm)  ","date":1534716000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534716000,"objectID":"a7540bb882b2d17918af62937d2b8cef","permalink":"http://martin-jung.github.io/post/2018_datascienceingooglecloud_nr2/","publishdate":"2018-08-20T00:00:00+02:00","relpermalink":"/post/2018_datascienceingooglecloud_nr2/","section":"post","summary":"In this new post I will go through my process of getting familiar with running R in the Google cloud and the posting sort of follows my previous post on getting started with the Google cloud. My dream setup would include to being able to switch seamless between running r code locally or in the cloud whenever I require more processing power. For instance similar doAzureParallel package available for Microsoft Azure.","tags":["cloud-computing","data","r","google-cloud"],"title":"Running rstudio in the Google cloud - [2]","type":"post"},{"authors":null,"categories":["cloud-computing"],"content":"In a previous post, I gave a brief introduction how to use google cloud compute to kickstart your cloud computing experience. While it is possible to run large spatial operations on google cloud compute, it is quite time-consuming to set up all the routines to load and process geospatial data. Luckily there is now a new platform (currently in beta-testing) called Google Earth Engine (GEE) described as planetary scale platform for spatial analyses. GEE is (currently) free to use and users can sign up for a beta-testing. There are a wide range of datasets already available within GEE, including all available Landsat, Sentinel and MODIS satellite data. GEE furthermore allows to pre-process these layers and the fantastatic google team also already pre-computed layers such as cloud masks or ingested fully radiometrically corrected layers. If the reader is interested in what is possible with GEE, have a look at the case studies on the GEE website.  In the rest of this post I will showcase some exemplary scripts I coded in Javascript which, besides python, is the primary way to access and pre-process data in GEE. I will mostly only comment on my script as there are extensive tutorials, videos and detailed API descriptions available for GEE. Whenever I load outside data (such as ESRI shapefiles) into GEE, I usually convert them to a KML file and then load them as Google Fusion Table (search online how to do this). All the (java-)scripts below can be pasted into the GEE code console, but if any errors occur then usually because of missing permissions (you might not be able to access my Google Fusion Tables). The following script quantifies the date of forest loss from the Hansen forest cover dataset\n// Load the global Hansen forest dataset var gfcImage = ee.Image(\u0026quot;UMD/hansen/global_forest_change_2017_v1_5\u0026quot;); /* @author Martin Jung - m.jung@sussex.ac.uk Idea: Get average date of forest loss within sampling extent */ // Coordinates and parameters // (This is a fusiontable id containing my polygon shapefile) var fullsites = ee.FeatureCollection('ft:1Oet2yGWvldNoVx8A6ZlCi75Ks_lrEG0dG9oD6k2j'); // All sites //////// Other Parameters ////////// var export_geometry = false; // export the geometry in the csv var scale = 30; // Resolution over which image collection should be reduced, 30 m = native scale var reduce = ee.Reducer.mean() // Reducer for image collection // Also available max(), mean(), median(), min(), mode(), or(), product(), sum(), stdDev() var what = \u0026quot;ForestLoss\u0026quot;; // Export name // ################################################################### // // Function and main CODE starts here // // ################################################################### // // Create TimeBand function createTimeBand(image) { return image.addBands(image.metadata('system:time_start')); } // ---- // // Select the band with loss year var forestImage = gfcImage.select(['lossyear']); // Mask out pixels with no lossyear var m = forestImage.gt(0); forestImage = forestImage.mask(m); // -------------------------------- // // Reduce per polygon per polygon var extracted = forestImage.reduceRegions(fullsites, reduce, scale); Export.table.toDrive({ collection: extracted.select([\u0026quot;.*\u0026quot;], null, export_geometry), description: \u0026quot;PREDICTSHansen_\u0026quot;+what, fileNamePrefix: \u0026quot;PREDICTSHansen_\u0026quot;+what, fileFormat: 'geoJSON' });  \nNot too difficult, right? Here is another script that calculates a 95 % percentile composite of three months of EVI data calculated from all available Landsat surface reflectance images from 1984 to 2017. My script furthermore exclude all pixels that are clearly covered by water (using data from Pekel et al. 2016 ) and mask out unsuitable images (too many clouds) as well as clouds and cloud-shadows. Finally the mean, composited EVI within a buffer was exported. Full script below: // Images and shapefiles // Including landsat Level 1 land-surface-reflectance bands var l8_led = ee.ImageCollection(\u0026quot;LANDSAT/LC08/C01/T1_SR\u0026quot;), l4_led = ee.ImageCollection(\u0026quot;LANDSAT/LT04/C01/T1_SR\u0026quot;), l5_led = ee.ImageCollection(\u0026quot;LANDSAT/LT05/C01/T1_SR\u0026quot;), l7_led = ee.ImageCollection(\u0026quot;LANDSAT/LE07/C01/T1_SR\u0026quot;), gsw = ee.Image(\u0026quot;JRC/GSW1_0/GlobalSurfaceWater\u0026quot;), // Global Surface water dataset usgadm = ee.FeatureCollection(\u0026quot;ft:1kA8n0e3lvUVuWJSixVf5WaPyeqskp8kZg9zU9TBX\u0026quot;), bbs_union = ee.FeatureCollection(\u0026quot;ft:1hqCRJam4b3niJZgRmJkOOMEe_qTwM77gZ-TyP4pe\u0026quot;), // Shapefiles I use my analyses. They contain polygons bbs = ee.FeatureCollection(\u0026quot;ft:1gUxInqbJAG-kcH6bn6PioNnZ6G7fZOBIIaEADnIO\u0026quot;); /* author: Martin Jung - 2018 (m.jung@sussex.ac.uk) Idea: Calculate avg overall EVI for all routes. */ // Parameters var scale = 30; // Resolution over which image collection should be reduced, 30m = native scale var precision = 10000; // Rounding precision var cloudlimit = 80; // Should images with that many clouds be kept ? In order to reduce effect on non-filtered shades var reduce = ee.Reducer.percentile([95]) ; // How should composites be reduced? var startmonth = 3; // start Month when to use images from var endmonth = 6; // end month var startday = 20; // start day var endday = 20; // end day var fname = \u0026quot;EVIstack\u0026quot;; // FileNamedDescription var what = 'evi'; // Name of resulting spectral index band // Expressions var f_evi = '2.5 * ((nir - red) / (nir + 2.4 * red + 1))'; // EVI2 formula (two-band version) // ################################################################### // // Function and main CODE starts here // // ################################################################### // // Function to mask excess EVI values defined as \u0026gt; 1 and \u0026lt; 0 var maskExcess = function(image) { var hi = image.lte(1); var lo = image.gte(0); var masked = image.mask(hi.and(lo)); return image.mask(masked); }; // Function to remove clouds - expects the new SR data to have a cfmask layer // 126122017 - Adapted to work with LT1 var maskclouds = function(scene) { // The \u0026quot;pixel_qa\u0026quot; band has various flags encoded in // different bits. We extract some of them as individual mask bands. // Note: Cloud masking information is present in \u0026quot;pixel_qa\u0026quot; // pixel_qa Bit 1: Clear pixel indicator. // pixel_qa Bit 2: Water indicator. // pixel_qa Bit 3: Cloud shadow indicator. // pixel_qa Bit 5: Cloud indicator. // pixel_qa Bits 6-7: Cloud confidence. // Fill = https://explorer.earthengine.google.com/#detail/LANDSAT%2FLE07%2FC01%2FT1_SR var clear = scene.select('pixel_qa').bitwiseAnd(2).neq(0); clear = scene.updateMask(clear); return(clear); }; // Water mask all pixels with over 90 % water occurence var water_mask = gsw.select('occurrence').gt(90).unmask(0); var watermask = function(image){ var masked = water_mask.eq(0); return image.mask(masked); }; // Create TimeBand function createTimeBand(image) { return image.addBands(image.metadata('system:time_start')); } // Filter out those bands with no images and create and empty image where there is none // (This function should not be necessary as the area I investigate has full availability of Landsat 4-8) var conditional = function(image) { return ee.Algorithms.If(ee.Number(image.get('num_elements')).gt(0), image, ee.Image(0).toDouble() .set('system:time_start',image.get('system:time_start')) .rename(what)); }; // VegIndex calculator. Calculate the EVI index (two-band versiob) function calcIndex(image){ var evi = image.expression( f_evi, { red: image.select('red').multiply(0.0001), // 620-670nm, RED nir: image.select('nir').multiply(0.0001) // 841-876nm, NIR }); // Rename that band to something appropriate var dimage = ee.Date(ee.Number(image.get('system:time_start'))).format(); return evi.select([0], [what]).set({'datef': dimage,'system:time_start': ee.Number(image.get('system:time_start'))}); } // ----------------------------------------- // // Further PROCESSING CODE STARTS BELOW // // ----------------------------------------- // // Load the polygon shapefiles for my analysis and use a bounding box to clip all outputs var bbs_filter = bbs_union; var bbox = bbs_filter.geometry().bounds(); Map.addLayer(bbs_filter);Map.addLayer(bbox) // Add to map and center to it // Filter the layers and set bounds // get the LC8 collection var L8 = l8_led .filterBounds(bbox) // filter all Landsat images by bound .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling .filterMetadata('CLOUD_COVER_LAND','less_than',cloudlimit) // Ignore images with too many clouds .map(maskclouds) // mask clouds and cloud-shadows from the image .map(watermask) // mask out water .map(createTimeBand); // add a time band // get the LE7 collection var L7 = l7_led .filterBounds(bbox) .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling .filterMetadata('CLOUD_COVER_LAND','less_than',cloudlimit) .map(maskclouds) .map(watermask) .map(createTimeBand); // get the LE5 collection var L5 = l5_led .filterBounds(bbox) .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling .filterMetadata('CLOUD_COVER_LAND','less_than',cloudlimit) .map(maskclouds) .map(watermask) .map(createTimeBand); // get the LE5 collection var L4 = l4_led .filterBounds(bbox) .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling .filterMetadata('CLOUD_COVER_LAND','less_than',cloudlimit) .map(maskclouds) .map(watermask) .map(createTimeBand); // Rename bands for all (note that band number change between Landsat satellites) var L4 = L4.map(function(image){ return image.select( ['B1','B2','B3','B4','B5','B7'], ['blue','green','red','nir','swir1','swir2'] ); }); var L5 = L5.map(function(image){ return image.select( ['B1','B2','B3','B4','B5','B7'], ['blue','green','red','nir','swir1','swir2'] ); }); var L7 = L7.map(function(image){ return image.select( ['B1','B2','B3','B4','B5','B7'], ['blue','green','red','nir','swir1','swir2'] ); }); var L8 = L8.map(function(image){ return image.select( ['B2','B3','B4','B5','B6','B7'], ['blue','green','red','nir','swir1','swir2'] ); }); // Merge the collections // this collection is sorted by time var Collection = ee.ImageCollection(L8.merge(L7)) .sort('system:time_start',true); Collection = ee.ImageCollection(Collection.merge(L5)) .sort('system:time_start',true); Collection = ee.ImageCollection(Collection.merge(L4)) .sort('system:time_start',true); // Calculate an vegetation index on full collection Collection = Collection.map( calcIndex ); // --------------------------------------------------- // // Mask out pixels with excess values (sensor errors) Collection = Collection.map( maskExcess ); // Clip to feature collection geometry Collection = Collection.map(function(i){return i.clip(bbs_filter);}); // Reduce the time series of images into a single image var img = Collection.reduce(reduce); // --------------------------------- // // Run function to calculate the mean per polygon var extract = function(img,bbs){ var extracted = img.reduceRegions(bbs, ee.Reducer.mean(), scale); return extracted; }; // The extracted results var results = extract(img, bbs); // Export the output Export.table.toDrive({ collection: results, folder: 'CropscapeTest', // Folder name in google drive description: 'Annual_' + fname + \u0026quot;_AvgEVI\u0026quot; + '_'+fname, fileNamePrefix : 'Annual_' + fname + \u0026quot;_AvgEVI\u0026quot; + '_'+fname, fileFormat: 'geoJSON' });  Hope these examples were helpful. I might post more of my code-examples at a later point.\nCheers, Martin\n","date":1532210400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532210400,"objectID":"ccc8c97b4f6bfc691a6167f4a8a307ce","permalink":"http://martin-jung.github.io/post/2018_googleearthengine-intro/","publishdate":"2018-07-22T00:00:00+02:00","relpermalink":"/post/2018_googleearthengine-intro/","section":"post","summary":"In a previous post, I gave a brief introduction how to use google cloud compute to kickstart your cloud computing experience. While it is possible to run large spatial operations on google cloud compute, it is quite time-consuming to set up all the routines to load and process geospatial data. Luckily there is now a new platform (currently in beta-testing) called Google Earth Engine (GEE) described as planetary scale platform for spatial analyses.","tags":["gis","cloud-computing","google-earth-engine","google-cloud","remote-sensing"],"title":"Introduction to Google Earth Engine","type":"post"},{"authors":null,"categories":["remote-sensing"],"content":" I won a prize :-) In this years Festival of Doctoral research at the University of Sussex I won the runner-up prize for the best image describing one\u0026rsquo;s doctoral research. The task was to submit a single image and 300 word abstract that visualizes and describes the goals of a persons doctoral research.\nPicture seen above (click here for larger version). Here is the accompanying description:\nTitle: Going back in time with satellites to assist biodiversity conservation  In my PhD I investigate how past land changes continue to affect local biodiversity. The above pictures were created from multiple satellite images (courtesy of the U.S. Geological Survey) and depict parts of the Roneam Daun Sam Wildlife sanctuary near the border (red line) of Thailand and Cambodia. In 2003 the sanctuary was reported to still have over 39 961 hectares of intact forest. Yet, on the 22nd of February 2018 the sanctuary has been officially dissolved by Cambodian royal degree owing to centuries of illegal timber harvesting. While the sanctuary has been ineffective in protecting its forest, it is unclear how strongly biodiversity in the area was and continues to be affected by these land changes. In my research I apply statistical models to link both satellite and local biodiversity survey data in order to quantify the magnitude of these changes in Cambodia and other areas globally.\n ","date":1530309600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530309600,"objectID":"7c11f3310c1c6ece4dd9f816981319be","permalink":"http://martin-jung.github.io/post/2018-phd-poster-prize/","publishdate":"2018-06-30T00:00:00+02:00","relpermalink":"/post/2018-phd-poster-prize/","section":"post","summary":"I won a prize :-) In this years Festival of Doctoral research at the University of Sussex I won the runner-up prize for the best image describing one\u0026rsquo;s doctoral research. The task was to submit a single image and 300 word abstract that visualizes and describes the goals of a persons doctoral research.\nPicture seen above (click here for larger version). Here is the accompanying description:\nTitle: Going back in time with satellites to assist biodiversity conservation  In my PhD I investigate how past land changes continue to affect local biodiversity.","tags":["gis","protected-areas","remote-sensing","landsat","phd"],"title":"PhD research image prize","type":"post"},{"authors":null,"categories":["data-science","cloud-computing"],"content":"Anyone analysing big data (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. A couple of years into my PhD and I increasingly realized that our little HPC suffers from the tragedy of the commons with more and more people requesting computation time on a few available nodes. That and also the tendency to have limited flexibility for running customized code (no root access, outdated modules and libraries, little space on the home drive to set up virtual environments, etc. \u0026hellip;) has made me quite frustrated and willing to switch to the \u0026ldquo;Cloud\u0026rdquo; for accessing computing resources. Cloud computing these days is well established, but mainly concentrated in the hands of three leading US firms. As far as I am aware one basically has to choose between Amazon AWS, Microsoft Azure and Google Cloud programs. Each have their own benefits and I leave it to the reader to search elsewhere for information on which one to chose. I picked the Google cloud free trial offer partly because of the following reasons:\n They have a 300$ give away. (I think Microsoft and Amazon offer sth. similar though)\n The free trial period lasts 12 months after which it runs out without incurring further cost. Furthermore there will remain a free-use contingent which can be exhausted for free. You fire up some use time on a f1-micro VM for instance.\n I am increasingly using Google\u0026rsquo;s Earth Engine platform and plan to use Google cloud storage to enhance my workflow.\n Private 1GB Git hosting (now especially useful since Competitor Microsoft has acquired Github )\n  That being said, I have also heard great things about AWS and Azure as well and might try them out at a later point as well.\n So here is how I started. My goal was to first get familiar with computing in the cloud and try to install some standard tools. Therefore First I fired up a micro instance Virtual Machine (which, in the google cloud, you can run over 700h each month for free). On the SSH button you have the opportunity to directly log into your cloud instance in the browser or in another ssh-client of you choosing. Each VM can be selected and also started / stopped or completly reseted in this screen as well (also via the \u0026rdquo;\u0026hellip;\u0026rdquo; button!) I\u0026rsquo;m going to install some basic data-science tools. Here is the entire thing as bash-script to be executed on the next, bigger, VM in a later stage ;-)\n# First lets install some necessary libraries sudo apt-get -y install bzip2 sudo apt-get -y install screen # Make a update and upgrade all, then clean up sudo apt-get update sudo apt-get -y upgrade sudo apt-get -y autoremove # Make download folder mkdir downloads cd downloads # Download anaconda wget https://repo.continuum.io/archive/Anaconda2-5.2.0-Linux-x86_64.sh # Install in the background (accept and updating any previous installations) bash Anaconda2-5.2.0-Linux-x86_64.sh -b -u -p $HOME/anaconda2 echo \u0026quot;export PATH=\\\u0026quot;~/anaconda2/bin:$PATH\\\u0026quot;\u0026quot; \u0026gt;\u0026gt; ~/.bashrc # Reload conf source ~/.bashrc # Install R # Add debian stretch repo and key, then install echo \u0026quot;deb http://cran.rstudio.com/bin/linux/debian stretch-cran35/\u0026quot; | sudo tee -a /etc/apt/sources.list sudo apt-key adv --keyserver keys.gnupg.net --recv-key 'E19F5F87128899B192B1A2C2AD5F960A256A04AF' sudo apt-get update sudo apt-get install -y r-base r-base-core r-base-dev sudo apt-get install -y libatlas3-base # Also install rstudio keyserver sudo apt-get -y install psmisc libssl-dev libcurl4-openssl-dev libssh2-1-dev wget https://download2.rstudio.org/rstudio-server-stretch-1.1.453-amd64.deb sudo dpkg -i rstudio-server-stretch-1.1.453-amd64.deb # Also install julia for later sudo apt-get -y install julia  Note to myself: For the future it might be easier to configure an analysis-ready docker image. Sth. to do for later\u0026hellip; Now we create a new configuration for a jupyter notebook and start it on the vm.\n# Create config jupyter notebook --generate-config # Add this to the configure echo \u0026quot;c = get_config()\u0026quot; \u0026gt;\u0026gt; ~/.jupyter/jupyter_notebook_config.py echo \u0026quot;c.NotebookApp.ip = '*'\u0026quot; \u0026gt;\u0026gt; ~/.jupyter/jupyter_notebook_config.py echo \u0026quot;c.NotebookApp.open_browser = False\u0026quot; \u0026gt;\u0026gt; ~/.jupyter/jupyter_notebook_config.py echo \u0026quot;c.NotebookApp.port = 8177\u0026quot; \u0026gt;\u0026gt; ~/.jupyter/jupyter_notebook_config.py # Set a password jupyter notebook password # Start up jupyter-notebook --no-browser --port=8177  The jupyter notebook can now be theoretically viewed in a browser. However we have to get access to the Google cloud intranet first. For this we will use the google cloud SDK, which you need to install on your local computer as well.\nThen execute for the google cloud sdk:\n# After installation: auth gcloud init # The open a SSH tunnel. For me that is: gcloud compute ssh --zone=us-central1-c --ssh-flag=\u0026quot;-D\u0026quot; --ssh-flag=\u0026quot;8177\u0026quot; --ssh-flag=\u0026quot;-N\u0026quot; --ssh-flag=\u0026quot;-n\u0026quot; wolkentest # If you have never done before, you will need to create a public/private ssh key  Now that you have created a SSH tunnel you can just open your local browser (ie. Chrome or similar) and navigate towards localhost:8177 and you should see your jupyter notebook. Happy computing! At the end, ensure that the VM is turned off, otherwise it will create ongoing costs!\n ","date":1529186400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529186400,"objectID":"573e4fe6582d6dfdf7072d2e95fcc129","permalink":"http://martin-jung.github.io/post/2018_datascienceingooglecloud/","publishdate":"2018-06-17T00:00:00+02:00","relpermalink":"/post/2018_datascienceingooglecloud/","section":"post","summary":"Anyone analysing big data (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. A couple of years into my PhD and I increasingly realized that our little HPC suffers from the tragedy of the commons with more and more people requesting computation time on a few available nodes.","tags":["cloud-computing","data","python","google-cloud"],"title":"Data science in the Google cloud - [1]","type":"post"},{"authors":null,"categories":["python","gis"],"content":" I perform most of my analyses using either R or standalone GDAL tools simply because of their general convenience and ease of use. Standard spatial analysis functions and tools are in my opinion still more readily available in R and most R packages are quite mature and well designed ( but see the readme ). Nevertheless python has caught up and a number of really helpful python modules for spatial analyses have been released to this date. I have always loved coding in python since I developed my LecoS plugin, especially because of it\u0026rsquo;s processing speed (doing any computation on numpy arrays is quite fun) and clean synthax. In this post I will demonstrate how to make use of the new (?) xarray (previously xray) python module to load remotely-sensed data and run \u0026ldquo;pixel-wise\u0026rdquo; correlation tests on them.\nI cannot release the shown spatial data, however any \u0026lsquo;stack\u0026rsquo; of remotely-sensed data or multiple satellite images will do as long as you can assign a time dimension/axis to the array.\nAll python modules should be easible downloadable via the package manager of your choice such as wheel, pip and conda \u0026amp; co.\n So what is xarray? In short it is a python package that enables explicit computation on labeled, multi-dimensional arrays, such as those commonly obtained from geophysical models or repeated satellite images. Think of it as a convenience wrapper that combines the best of pandas data structures with numpy\u0026rsquo;s array functionalities. It has been developed specifically with geophysical applications in mind and therefore should be quite useful for anyone using such data.\nLoading and plotting data So how does it work? Let\u0026rsquo;s get started.\nFirst load the default packages that we are going to use later.\n# Necessary defaults import numpy as np import matplotlib.pyplot as plt from scipy import stats import pandas as pd import os, sys # Xarray import xarray as xr  Next let us load some spatial data. In my example it is a stack of annual Landsat composite images created for the period 1984-2017. Each composite quantifies the Enhanced Vegetation Index (EVI) for a given year. My test data is roughly 100mB big, but all code presented can easily be scaled up to hundreds of GB. In the newest xarray version we can use the fantastic rasterio python interface to load commonly-used spatial data such as GeoTiffs.\nroute = \u0026quot;test_2063.tif\u0026quot; ds = xr.open_rasterio(route).rename({'x': 'Longitude', 'y': 'Latitude', 'band': 'year'})  Spatial-temporal satellite data comes with at least 3 dimensions. The spatial dimensions describing the rows and columns of the matrix as well as a third dimension usually associated with time (year, year-month, date, \u0026hellip;). xarray furthermore loads any spatial attributes (such as spatial extents, cell resolution or geographic projections) it can find and assigns them to the DataArray. Here is how my data looks: (note the attributes!)\n\u0026lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)\u0026gt; [69682660 values with dtype=float32] Coordinates: * year (year) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ... * Latitude (Latitude) float64 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 ... * Longitude (Longitude) float64 -87.94 -87.94 -87.94 -87.94 -87.94 -87.94 ... Attributes: transform: (-87.9403950755027, 0.0002694945852365922, 0.0, 32.599951492... crs: +init=epsg:4326 res: (0.0002694945852365922, 0.00026949458523998555) is_tiled: 0 nodatavals: (-3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38,...  We are going to the data and try to create a spatial visualization. xarray has inbuilt plotting capabilities to visualize both temporal and spatial data using matplotlib. In my example I first alter the temporal dimension to have the correct time axis. I furthermore convert my xarray DataArray to a DataSet (more here on the difference ) and also filter the array to encorporate only positive values.\n# Create a series with years dates = pd.date_range('1984-01-01', periods=34,freq = \u0026quot;A\u0026quot;).year ds.coords[\u0026quot;year\u0026quot;] = dates # Reset dates ds = ds.to_dataset(\u0026quot;EVI\u0026quot;) # Convert to dataset # Create a subset to run calcution on and correct scale + filter img = ds.pipe(lambda x: x * 0.0001) img = img.where(img \u0026gt;= 0) # Select the last item of year and plot it img.isel(year=33).EVI.plot(robust=True,cmap='viridis') # Finally plot a test year plt.title('Annual EVI in year %s' % ('2017') ) plt.ylabel('Latitude') plt.xlabel('Longitude')  Basic and applied calculations Next let us conduct some spatial-temporal analyses. The xarray created above can in principle easily be summarized as a whole or on the pixel level. Both can be easily achieved by grouping observations on the pixel level.\nds2.EVI.groupby('year').std().plot()  Equally if we want to quantify the standard deviation across all 34 years in the stack\nds2.EVI.groupby('Longitude','Latitude').std('year').plot()  But what if want to apply a defined method over each pixel in given xarray? This is again possible, but a little bit more complicated. First we need to define a function that performs the requested analysis and returns a single value. In my example I want to calculate Kendall\u0026rsquo;s rank correlation test for each time series over all pixels. Remember how we imported the scipy stats package at the start of this post? To save time and harddrive space, we simply want to know for this simple example whether EVI is \u0026ldquo;significantly\u0026rdquo; ($p = 0.05$) increasing over the whole time series. So let us build a function: def k_cor(x,y, pthres = 0.05, direction = True): \u0026quot;\u0026quot;\u0026quot; Uses the scipy stats module to calculate a Kendall correlation test :x vector: Input pixel vector to run tests on :y vector: The date input vector :pthres: Significance of the underlying test :direction: output only direction as output (-1 \u0026amp; 1) \u0026quot;\u0026quot;\u0026quot; # Check NA values co = np.count_nonzero(~np.isnan(x)) if co \u0026lt; 4: # If fewer than 4 observations return -9999 return -9999 # Run the kendalltau test tau, p_value = stats.kendalltau(x, y) # Criterium to return results in case of Significance if p_value \u0026lt; pthres: # Check direction if direction: if tau \u0026lt; 0: return -1 elif tau \u0026gt; 0: return 1 else: return tau else: return 0 # The function we are going to use for applying our kendal test per pixel def kendall_correlation(x,y,dim='year'): # x = Pixel value, y = a vector containing the date, dim == dimension return xr.apply_ufunc( k_cor, x , y, input_core_dims=[[dim], [dim]], vectorize=True, # !Important! output_dtypes=[int] )  Next we execute our new function and to do so we first need to create a new DataArray that contains the y variable (date in our example). Afterwards we can apply our function on the loaded xarray.\nx = xr.DataArray(np.arange(len(ds2['year']))+1, dims='year', coords={'year': ds2['year']}) r = kendall_correlation(ds2, x,'year')  This whole computation took us roughly 1min and 54 seconds on my Lenovo laptop.\nFurther speed ups In many cases an execution on a local computer hardly makes sense and is time inefficient. Particularly if your data is too large to fit into memory. This is usually the case with earth-observation data, which can easily become larger than 10GB++ .\nLuckily xarray supports parallel execution via the optional Dask integration. So what is dask? Compared to standard numpy array calculations it supports lazy evaluation of any supplied code. This means that your code is only executed on the dataset as soon as you tell dask+xarray to do so (via the compute() function ). Dask divides your array into many small pieces, so called chunks, each of which is presumed to be small enough to fit into memory. Chunking your data enables better parallelization which can easily be scaled up over multiple CPU cores or entire clusters of machines. So how do we enable dask for your computation ? This is actually quite simple.\n# Import dask stuff import dask.array as da from dask.diagnostics import ProgressBar # Then while loading in your data specify that you want your data to be loaded as chunks # A more optimal chunk size for your data can really speed up computation # So ensure that your dataset is correctly chuncked. ds = xr.open_rasterio(route,chunks={'band': 34, 'x': 1000, 'y': 1000}).rename({'x': 'Longitude', 'y': 'Latitude', 'band': 'year' })  You will notice that your DataArray has now become a dask-array\n\u0026lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)\u0026gt; dask.array\u0026lt;shape=(34, 1318, 1555), dtype=float32, chunksize=(34, 100, 100)\u0026gt; ...  Next you have to enable dask in your apply function and compute the result\ndef kendall_correlation(x,y,dim='year'): return xr.apply_ufunc( mk_cor, x , y, input_core_dims=[[dim], [dim]], vectorize=True, dask='parallelized', # Note the addition !!! output_dtypes=[int] ) # Make a little ProgressBar with ProgressBar(): # Until 'compute' is run, no computation is executed r = kendall_correlation(ds2, x,'year').compute()  Running my computation with dask decreased the whole processing time down to 45s !\nBut there is more we can do to further speed up the computation. By enabling numba as just-in-time (JIT) byte compiler for our kendall correlation function we can squeeze a couple more seconds out. Numba compiles your python function into a byte-compiled snip that is a lot faster to execute. One can enable the JIT compiler as follows:\nfrom numba import jit # Speedup for python functions @jit(nogil=True) # Enable JIT compiler def k_cor(x,y, pthres = 0.05, direction = True): ...  Note all this was run on a rather small dataset. The more data you have and the more computational intensive your analysis becomes, the more do the steps above improve your execution time.\nMy system info:\n Linux Ubuntu - Codename bionic Python 3.6.5 Numpy 1.13.3 Xarray 0.10.2 dask 0.16.0 numba 0.34.0  Full source code as gist\n ","date":1526940000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526940000,"objectID":"f5584c7cea8b7e65f29bf72c8bf2a317","permalink":"http://martin-jung.github.io/post/2018-xarrayregression/","publishdate":"2018-05-22T00:00:00+02:00","relpermalink":"/post/2018-xarrayregression/","section":"post","summary":"I perform most of my analyses using either R or standalone GDAL tools simply because of their general convenience and ease of use. Standard spatial analysis functions and tools are in my opinion still more readily available in R and most R packages are quite mature and well designed ( but see the readme ). Nevertheless python has caught up and a number of really helpful python modules for spatial analyses have been released to this date.","tags":["python","dask","xarray","satellite-imagery","time-series"],"title":"Robust correlations with xarray and dask","type":"post"},{"authors":null,"categories":null,"content":"","date":1526844288,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526844288,"objectID":"14dc7572bed2b0764d69cca9b24e5ff6","permalink":"http://martin-jung.github.io/project/greece/","publishdate":"2018-05-20T20:24:48+01:00","relpermalink":"/project/greece/","section":"project","summary":"I am contributing to a University of Sussex led research project that investigates the relationship between economic debt and changes in the environment. For this project I am analysing time series of satellite images to deduct statistics on how land has changed in Greece as a result of the economic recession after 2008.","tags":["research","gis"],"title":"The impact of economic recession on land changes in Greece","type":"project"},{"authors":null,"categories":null,"content":"","date":1517439600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517439600,"objectID":"67b595e874ae61775b1627239567c5db","permalink":"http://martin-jung.github.io/project/icarus/","publishdate":"2018-02-01T00:00:00+01:00","relpermalink":"/project/icarus/","section":"project","summary":"Utility functions to analyse spatial data in R","tags":["rstats"],"title":"Icarus","type":"project"},{"authors":null,"categories":[],"content":"Welcome to my new starting website. My name is Martin Jung and I consider myself an Environmental and data scientist. Read here for more.\nI have abandoned my previous online presence, the Conservation ecology blog since quite a while. Mostly because I was too preoccupied working on my PhD. However I also disliked the inflexibility of wordpress layouts and the added advertising below each wordpress.com article. Therefore the restart of my professional website powered by hugo and the academic theme. So feel welcome if you are a previous reader of my blog, but notice that I will put more emphasis on examples and demonstrations on this website.\nPlease do not hesitate to contact me.\nI\u0026rsquo;m particularly interested in scientific collaborations, consulting or data analyses offers. I also regularly give demonstrations and workshops on scientific data analysis (using R, python or QGIS), general data management (SQL database systems) and high performance computing. So please contact me if you think my services could be useful for your research group or company.\n","date":1497564000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497564000,"objectID":"df46741e34564e9fd5bcdbbe5ecf1df9","permalink":"http://martin-jung.github.io/post/2017-new-professional-website/","publishdate":"2017-06-16T00:00:00+02:00","relpermalink":"/post/2017-new-professional-website/","section":"post","summary":"Welcome to my new starting website. My name is Martin Jung and I consider myself an Environmental and data scientist. Read here for more.\nI have abandoned my previous online presence, the Conservation ecology blog since quite a while. Mostly because I was too preoccupied working on my PhD. However I also disliked the inflexibility of wordpress layouts and the added advertising below each wordpress.com article. Therefore the restart of my professional website powered by hugo and the academic theme.","tags":["general"],"title":"New professional website","type":"post"},{"authors":null,"categories":null,"content":"","date":1488322800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488322800,"objectID":"be29447964966eb720ff7ede0965afe9","permalink":"http://martin-jung.github.io/talk/2017_dacmtalk/","publishdate":"2017-03-01T00:00:00+01:00","relpermalink":"/talk/2017_dacmtalk/","section":"talk","summary":"I gave talk about the techniques used in my PhD and my workflow in analysing data","tags":["big data"],"title":"2017 Data analysis talk","type":"talk"},{"authors":["Olivia Norfolk"," Martin Jung"," Philip J Platts"," Phillista Malaki"," Dickens Odeny"," Robert Marchant"],"categories":null,"content":"","date":1487894400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487894400,"objectID":"59b9b0b876ec7368a92f80cd7b3da693","permalink":"http://martin-jung.github.io/publication/2017_birdsinthematrixafrica/","publishdate":"2017-02-24T00:00:00Z","relpermalink":"/publication/2017_birdsinthematrixafrica/","section":"publication","summary":"Agricultural conversion of tropical forests is a major driver of biodiversity loss. Slowing rates of deforestation is a conservation priority, but it is also useful to consider how species diversity is retained across the agricultural matrix. Here, we assess how bird diversity varies in relation to land use in the Taita Hills, Kenya. We used point counts to survey birds along a land‐use gradient that included primary forest, secondary vegetation, agroforest, timber plantation and cropland. We found that the agricultural matrix supports an abundant and diverse bird community with high levels of species turnover, but that forest specialists are confined predominantly to primary forest, with the matrix dominated by forest visitors. Ordination analyses showed that representation of forest specialists decreases with distance from primary forest. With the exception of forest generalists, bird abundance and diversity are lowest in timber plantations. Contrary to expectation, we found feeding guilds at similar abundances in all land‐use types. We conclude that whilst the agricultural matrix, and agroforest in particular, makes a strong contribution to observed bird diversity at the landscape scale, intact primary forest is essential for maintaining this diversity, especially amongst species of conservation concern. ","tags":["africa","birds","landscape","Taita","land-use"],"title":"Birds in the matrix: the role of agriculture in avian conservation in the Taita Hills, Kenya","type":"publication"},{"authors":null,"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"dcd8e9d468287b98cab7a803496be689","permalink":"http://martin-jung.github.io/project/bialowieza/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/project/bialowieza/","section":"project","summary":"Since my undergraduate thesis I have been both fascinated and concerned with the conservation of remaining ancient forests. Because of political developments surrounding Bialowieza forest,  I investigated the vegetation health of the remaining forest by building both a Landsat based forest-cover classification as well as a temporal disturbance mask for the year 2016.","tags":["research","gis"],"title":"Remotely sensed forest monitoring in Bialowieza forest","type":"project"},{"authors":["Martin Jung","Sam L.L. Hill","Phil J. Platts","Rob Marchant","Stefan Siebert","Anne Fournier","Fred B. Munyekenye","Andy Purvis","Neil D. Burgess","Tim Newbold"],"categories":null,"content":"","date":1482883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482883200,"objectID":"a809899e06ae6e000e8cd690fb5a8994","permalink":"http://martin-jung.github.io/publication/2017_mscthesisanimalconservation/","publishdate":"2016-12-28T00:00:00Z","relpermalink":"/publication/2017_mscthesisanimalconservation/","section":"publication","summary":"Land‐use change is the single biggest driver of biodiversity loss in the tropics. Biodiversity models can be useful tools to inform policymakers and conservationists of the likely response of species to anthropogenic pressures, including land‐use change. However, such models generalize biodiversity responses across wide areas and many taxa, potentially missing important characteristics of particular sites or clades. Comparisons of biodiversity models with independently collected field data can help us understand the local factors that mediate broad‐scale responses. We collected independent bird occurrence and abundance data along two elevational transects in Mount Kilimanjaro, Tanzania and the Taita Hills, Kenya. We estimated the local response to land use and compared our estimates with modelled local responses based on a large database of many different taxa across Africa. To identify the local factors mediating responses to land use, we compared environmental and species assemblage information between sites in the independent and African‐wide datasets. Bird species richness and abundance responses to land use in the independent data followed similar trends as suggested by the African‐wide biodiversity model, however the land‐use classification was too coarse to capture fully the variability introduced by local agricultural management practices. A comparison of assemblage characteristics showed that the sites on Kilimanjaro and the Taita Hills had higher proportions of forest specialists in croplands compared to the Africa‐wide average. Local human population density, forest cover and vegetation greenness also differed significantly between the independent and Africa‐wide datasets. Biodiversity models including those variables performed better, particularly in croplands, but still could not accurately predict the magnitude of local species responses to most land uses, probably because local features of the land management are still missed. Overall, our study demonstrates that local factors mediate biodiversity responses to land use and cautions against applying biodiversity models to local contexts without prior knowledge of which factors are locally relevant. ","tags":["thesis","fieldwork","land-use","modelling","validation","birds","ecology"],"title":"Local factors mediate the response of biodiversity to land use on two African mountains","type":"publication"},{"authors":["Lawrence N Hudson"," Tim Newbold"," Sara Contu"," Samantha LL Hill"," Igor Lysenko"," Adriana De Palma"," Helen RP Phillips","[...]","Martin Jung","[...]","Jörn P. W. Scharlemann","Andy Purvis"],"categories":null,"content":"","date":1481846400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481846400,"objectID":"b31acfd6e85e76133f7c6f6bc413ea42","permalink":"http://martin-jung.github.io/publication/2016_predicts-database-paper2/","publishdate":"2016-12-16T00:00:00Z","relpermalink":"/publication/2016_predicts-database-paper2/","section":"publication","summary":"The PREDICTS project—Projecting Responses of Ecological Diversity In Changing Terrestrial Systems (www.predicts.org.uk)—has collated from published studies a large, reasonably representative database of comparable samples of biodiversity from multiple sites that differ in the nature or intensity of human impacts relating to land use. We have used this evidence base to develop global and regional statistical models of how local biodiversity responds to these measures. We describe and make freely available this 2016 release of the database, containing more than 3.2 million records sampled at over 26,000 locations and representing over 47,000 species. We outline how the database can help in answering a range of questions in ecology and conservation biology. To our knowledge, this is the largest and most geographically and taxonomically representative database of spatial comparisons of biodiversity that has been collated to date; it will be useful to researchers and international efforts wishing to model and understand the global status of biodiversity.","tags":["predicts","database","ecology","land-use"],"title":"The database of the PREDICTS (Projecting Responses of Ecological Diversity In Changing Terrestrial Systems) project","type":"publication"},{"authors":null,"categories":null,"content":"","date":1477954800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477954800,"objectID":"44b45666c6fe6b34e6d7c4a6aa38b56d","permalink":"http://martin-jung.github.io/project/forestfrag/","publishdate":"2016-11-01T00:00:00+01:00","relpermalink":"/project/forestfrag/","section":"project","summary":"Globally tropical and temperate forests are getting increasingly more fragmented by both natural and anthropogenic causes. I quantified the global state of forest fragmentation across multiple years and investigated temporal patterns and concequences for biodiversity.","tags":["phd","research"],"title":"Quantifying global forest fragmentation","type":"project"},{"authors":["Tim Newbold"," Lawrence N Hudson"," Andrew P Arnell"," Sara Contu"," Adriana De Palma"," Simon Ferrier"," Samantha LL Hill"," Andrew J Hoskins"," Igor Lysenko"," Helen RP Phillips"," Victoria J Burton"," Charlotte WT Chng"," Susan Emerson"," Di Gao"," Gwilym Pask-Hale"," Jon Hutton"," Martin Jung"," Katia Sanchez-Ortiz"," Benno I Simmons"," Sarah Whitmee"," Hanbin Zhang"," Jörn PW Scharlemann"," Andy Purvis"],"categories":null,"content":"","date":1468540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468540800,"objectID":"5dc6c4d731ecf9c962e7ce82e4617f61","permalink":"http://martin-jung.github.io/publication/2016_biodiversityplanetaryboundary/","publishdate":"2016-07-15T00:00:00Z","relpermalink":"/publication/2016_biodiversityplanetaryboundary/","section":"publication","summary":"Land use and related pressures have reduced local terrestrial biodiversity, but it is unclear how the magnitude of change relates to the recently proposed planetary boundary (“safe limit”). We estimate that land use and related pressures have already reduced local biodiversity intactness—the average proportion of natural biodiversity remaining in local ecosystems—beyond its recently proposed planetary boundary across 58.1% of the world’s land surface, where 71.4% of the human population live. Biodiversity intactness within most biomes (especially grassland biomes), most biodiversity hotspots, and even some wilderness areas is inferred to be beyond the boundary. Such widespread transgression of safe limits suggests that biodiversity loss, if unchecked, will undermine efforts toward long-term sustainable development.","tags":["predicts","biodiversity","land-use","planetary boundary","extinction"],"title":"Has land use pushed terrestrial biodiversity beyond the planetary boundary? A global assessment","type":"publication"},{"authors":["Philip Martin","Martin Jung","Francis Q. Bearley","Relena R. Ribbons","Emily R. Lines","Aerin L. Jacob"],"categories":null,"content":"","date":1454544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454544000,"objectID":"24247d875d9dd86bf8f0cd4d247e151a","permalink":"http://martin-jung.github.io/publication/2016_matureforestspeerj/","publishdate":"2016-02-04T00:00:00Z","relpermalink":"/publication/2016_matureforestspeerj/","section":"publication","summary":"Globally, mature forests appear to be increasing in biomass density (BD). There is disagreement whether these increases are the result of increases in atmospheric CO2 concentrations or a legacy effect of previous land-use. Recently, it was suggested that a threshold of 450 years should be used to define mature forests and that many forests increasing in BD may be younger than this. However, the study making these suggestions failed to account for the interactions between forest age and climate. Here we revisit the issue to identify: (1) how climate and forest age control global forest BD and (2) whether we can set a threshold age for mature forests. Using data from previously published studies we modelled the impacts of forest age and climate on BD using linear mixed effects models. We examined the potential biases in the dataset by comparing how representative it was of global mature forests in terms of its distribution, the climate space it occupied, and the ages of the forests used. BD increased with forest age, mean annual temperature and annual precipitation. Importantly, the effect of forest age increased with increasing temperature, but the effect of precipitation decreased with increasing temperatures. The dataset was biased towards northern hemisphere forests in relatively dry, cold climates. The dataset was also clearly biased towards forests ","tags":["forests","carbon","modelling"],"title":"Can we set a global threshold age to define mature forests?","type":"publication"},{"authors":["Martin Jung"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f7e0f2b14c50863a4397bfd05d0f2c80","permalink":"http://martin-jung.github.io/publication/2016_lecos/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/publication/2016_lecos/","section":"publication","summary":"The quantification of landscape structures from remote-sensing products is an important part of many analyses in landscape ecology studies. This paper introduces a new free and open- source tool for conducting landscape ecology analysis. LecoS is able to compute a variety of basic and advanced landscape metrics in an automatized way. The calculation can furthermore be partitioned by iterating through an optional provided polygon layer. The new tool is integrated into the QGIS processing framework and can thus be used as a stand- alone tool or within bigger complex models. For illustration a potential case-study is presented, which tries to quantify pollinator responses on landscape derived metrics at various scales","tags":["python","gis","qgis","landscape"],"title":"LecoS - A python plugin for automated landscape ecology analysis","type":"publication"},{"authors":null,"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451602800,"objectID":"0660f9bbc7782b7a4c2a49e8204daa47","permalink":"http://martin-jung.github.io/talk/2016_sussexgittalk/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/talk/2016_sussexgittalk/","section":"talk","summary":"I gave a short presentation of the merits of using github for collaboration and archiving","tags":["coding","reproducability","data"],"title":"2016 Introducing github at University of Sussex","type":"talk"},{"authors":null,"categories":null,"content":"","date":1446332400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446332400,"objectID":"770784d7f8b89e84bfac271f1c8d1ce6","permalink":"http://martin-jung.github.io/talk/2015_predictstalk/","publishdate":"2015-11-01T00:00:00+01:00","relpermalink":"/talk/2015_predictstalk/","section":"talk","summary":"Presentation about my MSc thesis given at the annual PREDICTS conference","tags":["predicts","africa","biodiversity","land use"],"title":"2015 PREDICTS conference talk","type":"talk"},{"authors":null,"categories":null,"content":" Aim Understanding biodiversity impacts of land use dynamics\nSummary Humans have already affected about 75% of the Earth’s ice-free terrestrial surface. Land use change is one of the largest threats to biodiversity conservation. Despite this realisation, our understanding of what aspects and how land use/land cover (LULC) changes impact biodiversity is relatively limited. This PhD project will use the largest, global database on matched biodiversity observations—containing over 2 million records representing 35,000 species from 20,000 sites across 88 countries (www.predicts.org.uk)—and combine this with freely-available, long-term satellite data of LULC change (e.g. MODIS, Landsat), to understand which aspects of LULC change are most important to determine biodiversity outcomes. Such understanding will allow projecting biodiversity outcomes in space and time, and guide LULC decisions to minimize biodiversity impacts. Past ecological research on LULC change has focussed mostly on single events, and mostly on conversions from natural to human-dominated lands, e.g. deforestation, cropland expansion. However, in reality human use of land is a dynamic process of LULC changing and alternating through time. In particular, once natural lands are converted they tend to cycle through various human uses, abandonment, and sometimes regrowth. Time-series of remotely-sensed imagery enable us to study the frequency, sequence, time span and magnitude of LULC changes for most locations on the Earth’s surface. Different types and transitions of LULC will have different impacts on biodiversity, e.g. changing wheat to barley may be equivalent for some species, whereas changing forest to pasture may not. Further, some impacts on biodiversity of land use dynamics can be observed immediately, e.g. for sessile species, whereas other species tend to show a temporal lag in response, e.g. for species that can disperse. Combining best available global biodiversity and remotely-sensed data we will investigate which aspects of LULC dynamics, such as habitat stability, shocks (e.g. fire, drought, deforestation), phenological shifts, are important to determine biodiversity outcomes, and also investigate species-energy relationships. This understanding will help plan conservation actions, e.g. by altering the sequence of LULC changes more or less biodiversity may be conserved; allow extrapolation of biodiversity impacts in space and time; and ultimately feed into policy processes, such as the reporting to the Convention on Biological Diversity and IPBES.\nObjectives and core chapters  Investigate if past differences in land use and land cover can explain current differences in local species assemblages Quantifying the general impact of past disturbance events on local species assemblages Assess if differing histories of land changes drive species assemblage assembly Investigate whether changes in local biodiversity are correlated with landscape-wide land changes  ","date":1442275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442275200,"objectID":"c9a66b2d3f935438f991c02f81127b35","permalink":"http://martin-jung.github.io/project/phd/","publishdate":"2015-09-15T00:00:00Z","relpermalink":"/project/phd/","section":"project","summary":"Short summary of the research aims for my PhD thesis","tags":["phd-thesis"],"title":"My PhD research aims","type":"project"},{"authors":["Lawrence N Hudson"," Tim Newbold"," Sara Contu"," Samantha LL Hill"," Igor Lysenko"," Adriana De Palma"," Helen RP Phillips"," Rebecca A Senior","[...]"," Martin Jung","[...]","Jörn P. W. Scharlemann","Andy Purvis"],"categories":null,"content":"","date":1417478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417478400,"objectID":"1be5ec1b631cd64ef41a57adf4c707fe","permalink":"http://martin-jung.github.io/publication/2014_predicts-database-paper1/","publishdate":"2014-12-02T00:00:00Z","relpermalink":"/publication/2014_predicts-database-paper1/","section":"publication","summary":"Biodiversity continues to decline in the face of increasing anthropogenic pressures such as habitat destruction, exploitation, pollution and introduction of alien species. Existing global databases of species’ threat status or population time series are dominated by charismatic species. The collation of datasets with broad taxonomic and biogeographic extents, and that support computation of a range of biodiversity indicators, is necessary to enable better understanding of historical declines and to project – and avert – future declines. We describe and assess a new database of more than 1.6 million samples from 78 countries representing over 28,000 species, collated from existing spatial comparisons of local‐scale biodiversity exposed to different intensities and types of anthropogenic pressures, from terrestrial sites around the world. The database contains measurements taken in 208 (of 814) ecoregions, 13 (of 14) biomes, 25 (of 35) biodiversity hotspots and 16 (of 17) megadiverse countries. The database contains more than 1% of the total number of all species described, and more than 1% of the described species within many taxonomic groups – including flowering plants, gymnosperms, birds, mammals, reptiles, amphibians, beetles, lepidopterans and hymenopterans. The dataset, which is still being added to, is therefore already considerably larger and more representative than those used by previous quantitative models of biodiversity trends and responses. The database is being assembled as part of the PREDICTS project (Projecting Responses of Ecological Diversity In Changing Terrestrial Systems – www.predicts.org.uk). We make site‐level summary data available alongside this article. The full database will be publicly available in 2015. ","tags":["predicts","database","ecology","land-use","modelling"],"title":"The PREDICTS database: a global database of how local terrestrial biodiversity responds to human impacts","type":"publication"},{"authors":null,"categories":null,"content":"","date":1399240800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1399240800,"objectID":"0969b5597de374c2eaf6eab860f40a92","permalink":"http://martin-jung.github.io/project/spsqlite/","publishdate":"2014-05-05T00:00:00+02:00","relpermalink":"/project/spsqlite/","section":"project","summary":"Start of development of a ROpenSCI package to load and query SQLite spatial layers in R. Still in alpha-stage","tags":["rstats"],"title":"Spsqlite","type":"project"},{"authors":null,"categories":null,"content":"","date":1388530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388530800,"objectID":"275e499e17f793ee8cc1a5ad261a518c","permalink":"http://martin-jung.github.io/project/biofrag/","publishdate":"2014-01-01T00:00:00+01:00","relpermalink":"/project/biofrag/","section":"project","summary":"I assisted in programmed and setting up a relational database for the BIOFRAG project. Involved the configuration of a PostgreSQL+POSTGIS environment and writing of multiple sql queries.","tags":["research","data"],"title":"Biofrag","type":"project"},{"authors":null,"categories":null,"content":"","date":1388530800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388530800,"objectID":"86e797384a5cbdeded315e41ae020732","permalink":"http://martin-jung.github.io/project/marfunky/","publishdate":"2014-01-01T00:00:00+01:00","relpermalink":"/project/marfunky/","section":"project","summary":"Simple Github based R package with helper functions for data wrangling and formatting.","tags":["coding","rstats"],"title":"Marfunky","type":"project"},{"authors":null,"categories":null,"content":"","date":1369000800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369000800,"objectID":"d0b2adf9b3e9159dda4b3d8f9c4c43d3","permalink":"http://martin-jung.github.io/talk/2013_lundlandscapeecology/","publishdate":"2013-05-20T00:00:00+02:00","relpermalink":"/talk/2013_lundlandscapeecology/","section":"talk","summary":"I was invited by Dr. Klaus Birkhofer from the University of Lund to give a practical workshop in landscape ecology analysis with QGIS for the Biodiversity and Conservation Science Group in which around 13 people participated","tags":["qgis","gis","lecos","landscape-ecology"],"title":"2013 - QGIS and Landscape ecology workshop at Lund","type":"talk"},{"authors":["Martin Jung"],"categories":null,"content":"","date":1348358400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1348358400,"objectID":"bd2fdbfa9c45ff1adc4265db9bad8383","permalink":"http://martin-jung.github.io/publication/2012_bscthesis/","publishdate":"2012-09-23T00:00:00Z","relpermalink":"/publication/2012_bscthesis/","section":"publication","summary":"Despite increasing concerns about a loss of pollinators, there is little knowledge about how habitat fragmentation affects temperate plant-pollinators interactions. Here we investigate local habitat quality and landscape-wide effects of forest fragmentation on the specialization and diversity of quantitative temperate plant-pollinator networks. We sampled flower visitors along a forest cover and landscape context gradient in the last primeval forest of European lowland, in Białowieża (Eastern Poland). We hypothesized that diversity and specialization of plant-pollinator networks show hump-shaped reactions to intermediate levels of fragmen- tation, while on the local scale declining habitat quality influences the pollinator community composition and visitation rate. During 121 observation hours we sampled 4.929 flower visitors from 353 different pollinator species on 16 focal plant species. We found pollinator diversity to show a hump-shaped response to forest cover on the landscape level. However, plant specialization did not vary with forest cover but decreased with increasing flower abundance of con-specific plants in the vicinity of the focal plants. On the local scale overall pollinator visitation rate decreased with flower abundance, while diversity increased with a greater number of flowers. We furthermore found the pollinator communities per plant species to be influenced by canopy cover in the vicinity, while pooled communities per study site showed a significant relation to forest cover and focal plant diversity. We assume that additional floral resources in the vicinity are effectively used by pollina- tors and therefore the intensity of forest fragmentation effects is buffered. Forest pollinators seem to be capable of shifting their floral preferences to these additional resources in case of forest fragmentation. Patchy distributed floral resources and higher flower abundance in the vicinity may further stabilize pollination interactions in Białowieżas fragmented forest parts. This highlights the importance of local habitat quality for forest pollinators and might enhance future conservation measures. Our findings indicate that forest fragmentation effects at landscape level might be buffered by local habitat quality such as floral resources and nesting opportunities (to a certain extent).","tags":["ecology","thesis","plant-pollinator","networks","fieldwork"],"title":"Influence of Forest Fragmentation on Temperate Plant-pollinator Networks","type":"publication"},{"authors":["Martin Jung","Felix Gaßner","Lars Gorschlüter","Karin Lamsfuß"],"categories":null,"content":"","date":1330560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1330560000,"objectID":"95ff483a008dbcc97860183031e2f528","permalink":"http://martin-jung.github.io/publication/2012_save-palmoil-report/","publishdate":"2012-03-01T00:00:00Z","relpermalink":"/publication/2012_save-palmoil-report/","section":"publication","summary":"Report written pro-bono for the SAVE Wildlife foundation (https://save-wildlife.org)","tags":["palmoil","africa","conservation"],"title":"The U.S. investors and African palm oil - Rainforest clearing and local resistance in West Cameroon","type":"publication"},{"authors":null,"categories":null,"content":"","date":1325372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325372400,"objectID":"d0b8106eeaa06ef9b48290189244bac0","permalink":"http://martin-jung.github.io/project/lecos/","publishdate":"2012-01-01T00:00:00+01:00","relpermalink":"/project/lecos/","section":"project","summary":"LecoS - Land cover statistics is plugin for the QGIS GIS software suite. It converts classified raster layers to arrays using the powerful numpy library. Based on a Connected Component Labeling approach it further identifies class patches and calculates landscape metrics. The use can choose to calculate single or several metrics for the raster classes.","tags":["qgis","gis","python","coding"],"title":"LecoS","type":"project"},{"authors":null,"categories":null,"content":"","date":1325372400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325372400,"objectID":"11d25b135f5635a8dd95b8019d5a152f","permalink":"http://martin-jung.github.io/project/qsdm/","publishdate":"2012-01-01T00:00:00+01:00","relpermalink":"/project/qsdm/","section":"project","summary":"During my MSc course I programmed a little experimental python plugin through which several macroecological functions became available for GIS users. It furthermore acts as a wrapper for calculating a maximum entropy model (MaxEnt) within QGIS.","tags":["gis","coding","python"],"title":"QSDM","type":"project"},{"authors":null,"categories":null,"content":"","date":946681200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946681200,"objectID":"f5035b6d6bd761c5e996d8d700ee63f2","permalink":"http://martin-jung.github.io/project/birdlookup/","publishdate":"2000-01-01T00:00:00+01:00","relpermalink":"/project/birdlookup/","section":"project","summary":"A small little tool that translates the names of any entered bird from German to English and Latin.","tags":["python","coding"],"title":"Birdlookup","type":"project"}]