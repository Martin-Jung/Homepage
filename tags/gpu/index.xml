<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gpu on Martin Jung - Environmental and data scientist</title>
    <link>http://martin-jung.github.io/tags/gpu/</link>
    <description>Recent content in gpu on Martin Jung - Environmental and data scientist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Martin Jung 2017-2018 &amp;bull; &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;i class=&#34;fa fa-cc&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;&lt;/a&gt;</copyright>
    <lastBuildDate>Fri, 16 Nov 2018 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="http://martin-jung.github.io/tags/gpu/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using the GPU for gradient descent boosting</title>
      <link>http://martin-jung.github.io/post/2018_xgbboostcuda/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_xgbboostcuda/</guid>
      <description>Running machine learning algorithms on large amounts of data can take considerable time. There are multiple ways of speeding up your code. The most obvious is to properly parallize your code and/or - assuming R or python is used - replace certain functions that cause a bottleneck to faster languages such as C++ or Julia. What also works is to simply have more computational power. In a previous post I elaborated on how to make use of the google cloud processing platform.</description>
    </item>
    
  </channel>
</rss>