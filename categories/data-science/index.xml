<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data-science on Martin Jung</title>
    <link>http://martin-jung.github.io/categories/data-science/</link>
    <description>Recent content in Data-science on Martin Jung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Martin Jung - &amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 17 Jun 2018 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="http://martin-jung.github.io/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data science in the Google cloud - [1]</title>
      <link>http://martin-jung.github.io/2018/data-science-in-the-google-cloud---1/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2018/data-science-in-the-google-cloud---1/</guid>
      <description>Anyone analysing big data (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. Yet our little HPC increasingly suffers from the tragedy of the commons with more and more people requesting computation time on the few available nodes.</description>
    </item>
    
  </channel>
</rss>