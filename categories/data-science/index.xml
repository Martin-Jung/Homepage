<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-science on Martin Jung - Environmental and data scientist</title>
    <link>http://martin-jung.github.io/categories/data-science/</link>
    <description>Recent content in data-science on Martin Jung - Environmental and data scientist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Martin Jung 2017-2018 &amp;bull; &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;i class=&#34;fa fa-cc&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 20 Aug 2018 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="http://martin-jung.github.io/categories/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Running rstudio in the Google cloud - [2]</title>
      <link>http://martin-jung.github.io/post/2018_datascienceingooglecloud_nr2/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_datascienceingooglecloud_nr2/</guid>
      <description>In this new post I will go through my process of getting familiar with running R in the Google cloud and the posting sort of follows my previous post on getting started with the Google cloud. My dream setup would include to being able to switch seamless between running r code locally or in the cloud whenever I require more processing power. For instance similar doAzureParallel package available for Microsoft Azure.</description>
    </item>
    
    <item>
      <title>Data science in the Google cloud - [1]</title>
      <link>http://martin-jung.github.io/post/2018_datascienceingooglecloud/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_datascienceingooglecloud/</guid>
      <description>Anyone analysing big data (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. A couple of years into my PhD and I increasingly realized that our little HPC suffers from the tragedy of the commons with more and more people requesting computation time on a few available nodes.</description>
    </item>
    
  </channel>
</rss>