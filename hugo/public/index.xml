<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Martin Jung on Martin Jung</title>
    <link>http://martin-jung.github.io/</link>
    <description>Recent content in Martin Jung on Martin Jung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Martin Jung - &amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>DataScienceCloud_v2</title>
      <link>http://martin-jung.github.io/2018/datasciencecloud_v2/</link>
      <pubDate>Tue, 26 Jun 2018 23:15:30 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2018/datasciencecloud_v2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DataScienceCloud_v2</title>
      <link>http://martin-jung.github.io/2018/datasciencecloud_v2/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2018/datasciencecloud_v2/</guid>
      <description>&lt;p&gt;Finally let us start rstudio server to run on the Google cloud. Normally you would need to set up the rstudio server manually, but with the new &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/index.html&#34; target=&#34;_blank&#34;&gt;googleComputeEngineR&lt;/a&gt; package this whole process has become a lot easier
&lt;br /&gt;
So let&amp;rsquo;s setup googleComputeEngineR. First we need to create a credentials file. For my project and test cloud-instance I create my file on my local system (&lt;strong&gt;!&lt;/strong&gt;) like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create the file
touch ~/.Renviron
echo &amp;quot;GCE_AUTH_FILE=\&amp;quot;~/wasserdampf.json\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
echo &amp;quot;GCE_DEFAULT_PROJECT_ID=\&amp;quot;wolke7-208420\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
echo &amp;quot;GCE_DEFAULT_ZONE=\&amp;quot;us-central1-a\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;
Now after starting R and installing &lt;strong&gt;googleComputeEngineR&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
             name = &amp;quot;my-rstudio&amp;quot;,
             username = &amp;quot;martin&amp;quot;, password = &amp;quot;test&amp;quot;,
             predefined_type = &amp;quot;f1-micro&amp;quot; # For a micro machine
#             cpus = &amp;quot;16&amp;quot;, memory = &amp;quot;34&amp;quot;
             )

# To stop
gce_vm_stop(vm)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Data science in the Google cloud - [1]</title>
      <link>http://martin-jung.github.io/2018/data-science-in-the-google-cloud---1/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2018/data-science-in-the-google-cloud---1/</guid>
      <description>

&lt;p&gt;Anyone analysing &lt;strong&gt;big data&lt;/strong&gt; (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. Yet our little HPC increasingly suffers from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Tragedy_of_the_commons&#34; target=&#34;_blank&#34;&gt;tragedy of the commons&lt;/a&gt; with more and more people requesting computation time on the few available nodes.  That and also the tendency to have limited flexibility for running customized code (no root access, outdated modules and libraries, little space on the home drive to set up virtual environments, etc. &amp;hellip;) has made me quite frustrated and willing to switch to the &amp;ldquo;Cloud&amp;rdquo; for accessing computing resources.
&lt;/p&gt;

&lt;p&gt;Cloud computing these days is well established, but mainly concentrated in the hands of three leading US firms. As far as I am aware one basically has to choose between Amazon AWS, Microsoft Azure and Google Cloud programs. Each have their own benefits and I leave it to the reader to search elsewhere for information on which one to chose.
&lt;/p&gt;

&lt;p&gt;I picked the &lt;a href=&#34;https://cloud.google.com/free-trial/&#34; target=&#34;_blank&#34;&gt;Google cloud free trial offer&lt;/a&gt; partly because of the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They have a 300$ give away. (I think Microsoft and Amazon offer sth. similar though)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The free trial period lasts 12 months after which it runs out without incurring further cost. Furthermore there will remain a &lt;a href=&#34;https://cloud.google.com/free/&#34; target=&#34;_blank&#34;&gt;free-use contingent&lt;/a&gt; which can be exhausted for free. You fire up some use time on a f1-micro VM for instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I am increasingly using &lt;a href=&#34;https://earthengine.google.com/&#34; target=&#34;_blank&#34;&gt;Google&amp;rsquo;s Earth Engine platform&lt;/a&gt; and plan to use Google cloud storage to enhance my workflow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Private 1GB Git hosting (now especially useful since Competitor Microsoft has acquired Github )&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That being said, I have also heard great things about AWS and Azure as well and might try them out at a later point as well.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;So here is how I started. My goal was to first get familiar with computing in the cloud and try to install some standard tools. Therefore
First I fired up a micro instance &lt;strong&gt;V&lt;/strong&gt;irtual &lt;strong&gt;M&lt;/strong&gt;achine (which, in the google cloud, you can run over 700h each month for free).
&lt;img src=&#34;./img/posts/GoogleCloudInstance.png&#34; alt=&#34;Micro instance in Google cloud &#34; /&gt;
On the SSH button you have the opportunity to directly log into your cloud instance in the browser or in another ssh-client of you choosing.
Each VM can be selected and also started / stopped or completly reseted in this screen as well (also via the &lt;strong&gt;&lt;em&gt;&amp;rdquo;&amp;hellip;&amp;rdquo;&lt;/em&gt;&lt;/strong&gt; button!)
&lt;br /&gt;
I&amp;rsquo;m going to install some basic data-science tools.
Here is the entire thing as bash-script to be executed on the next, bigger, VM in a later stage ;-)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# First lets install some necessary libraries
sudo apt-get install bzip2

# Make a update and upgrade all, then clean up
sudo apt-get update
sudo apt-get -y upgrade
sudo apt-get -y autoremove

# Make download folder
mkdir downloads
cd downloads
# Download anaconda
wget https://repo.continuum.io/archive/Anaconda2-5.2.0-Linux-x86_64.sh
# Install in the background (accept and updating any previous installations)
bash Anaconda2-5.2.0-Linux-x86_64.sh -b -u -p $HOME/anaconda2
# Reload conf
source ~/.bashrc

# Install R
# Add debian stretch repo and key, then install
echo &amp;quot;deb http://cran.rstudio.com/bin/linux/debian stretch-cran35/&amp;quot; | sudo tee -a /etc/apt/sources.list
sudo apt-key adv --keyserver keys.gnupg.net --recv-key &#39;E19F5F87128899B192B1A2C2AD5F960A256A04AF&#39;
sudo apt-get update
sudo apt-get install -y r-base r-base-core r-base-dev
sudo apt-get install -y libatlas3-base

# Also install rstudio keyserver
sudo apt-get -y install psmisc libssl-dev libcurl4-openssl-dev libssh2-1-dev
wget https://download2.rstudio.org/rstudio-server-stretch-1.1.453-amd64.deb
sudo dpkg -i rstudio-server-stretch-1.1.453-amd64.deb

# Also install julia for later
sudo apt-get -y install julia

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note to myself&lt;/strong&gt;: For the future it might be easier to configure an analysis-ready docker image
&lt;/p&gt;

&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;At the end, ensure that the VM is turned off, otherwise it will create ongoing costs!&lt;/p&gt;

&lt;/div&gt;


&lt;h3 id=&#34;memo&#34;&gt;Memo&lt;/h3&gt;

&lt;p&gt;As I read more, I learnt that there are way more convenient ways of getting started with cloud computing, especially for R ;) &lt;br /&gt;
See my &lt;a href=&#34;http://martin-jung.github.io/2018/datasciencecloud_v2/&#34; target=&#34;_blank&#34;&gt;next post&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust correlations with xarray and dask</title>
      <link>http://martin-jung.github.io/2018/robust-correlations-with-xarray-and-dask/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2018/robust-correlations-with-xarray-and-dask/</guid>
      <description>

&lt;p&gt;I perform most of my analyses using either &lt;strong&gt;R&lt;/strong&gt; or standalone &lt;strong&gt;GDAL&lt;/strong&gt; tools simply because of their general convenience and ease of use. Standard spatial analysis functions and tools are in my opinion still more readily available in R and most R packages are quite mature and well designed ( &lt;a href=&#34;http://www.rspatial.org/&#34; target=&#34;_blank&#34;&gt;but see the readme&lt;/a&gt; ). Nevertheless &lt;strong&gt;python&lt;/strong&gt; has caught up and a number of really helpful python modules for spatial analyses have been released to this date. I have always loved coding in python since I developed my &lt;a href=&#34;https://github.com/Martin-Jung/LecoS&#34; target=&#34;_blank&#34;&gt;LecoS plugin&lt;/a&gt;, especially because of it&amp;rsquo;s processing speed (doing any computation on numpy arrays is quite fun) and clean synthax. In this post I will demonstrate how to make use of the new (?) &lt;a href=&#34;http://xarray.pydata.org&#34; target=&#34;_blank&#34;&gt;xarray&lt;/a&gt; (previously xray) python module to load remotely-sensed data and run &amp;ldquo;pixel-wise&amp;rdquo; correlation tests on them.&lt;/p&gt;

&lt;p&gt;I cannot release the shown spatial data, however any &amp;lsquo;stack&amp;rsquo; of remotely-sensed data or multiple satellite images will do as long as you can assign a time dimension/axis to the array.&lt;/p&gt;

&lt;p&gt;All python modules should be easible downloadable via the package manager of your choice such as wheel, pip and conda &amp;amp; co.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;So what is &lt;strong&gt;xarray&lt;/strong&gt;? In short it is a python package that enables explicit computation on labeled, multi-dimensional arrays, such as those commonly obtained from geophysical models or repeated satellite images. Think of it as a convenience wrapper that combines the best of &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34;&gt;pandas&lt;/a&gt; data structures with numpy&amp;rsquo;s array functionalities. It has been developed specifically with geophysical applications in mind and therefore should be quite useful for anyone using such data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xarray.pydata.org/en/stable/_images/dataset-diagram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;loading-and-plotting-data&#34;&gt;Loading and plotting data&lt;/h3&gt;

&lt;p&gt;So how does it work? Let&amp;rsquo;s get started.&lt;br /&gt;
First load the default packages that we are going to use later.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Necessary defaults
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import pandas as pd
import os, sys

# Xarray
import xarray as xr

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next let us load some spatial data. In my example it is a stack of annual &lt;strong&gt;Landsat&lt;/strong&gt; composite images created for the period 1984-2017. Each composite quantifies the Enhanced Vegetation Index (&lt;strong&gt;EVI&lt;/strong&gt;) for a given year. My test data is roughly 100mB big, but all code presented can easily be scaled up to hundreds of GB. &lt;br /&gt;
In the newest xarray version we can use the fantastic &lt;strong&gt;rasterio&lt;/strong&gt; python interface to load commonly-used spatial data such as GeoTiffs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;route = &amp;quot;test_2063.tif&amp;quot;
ds = xr.open_rasterio(route).rename({&#39;x&#39;: &#39;Longitude&#39;, &#39;y&#39;: &#39;Latitude&#39;, &#39;band&#39;: &#39;year&#39;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spatial-temporal satellite data comes with at least 3 dimensions. The spatial dimensions describing the rows and columns of the matrix as well as a third dimension usually associated with time (year, year-month, date, &amp;hellip;). xarray furthermore loads any spatial attributes (such as spatial extents, cell resolution or geographic projections) it can find and assigns them to the DataArray. &lt;br /&gt;
Here is how my data looks: (note the attributes!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)&amp;gt;
[69682660 values with dtype=float32]
Coordinates:
  * year       (year) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...
  * Latitude   (Latitude) float64 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 ...
  * Longitude  (Longitude) float64 -87.94 -87.94 -87.94 -87.94 -87.94 -87.94 ...
Attributes:
    transform:   (-87.9403950755027, 0.0002694945852365922, 0.0, 32.599951492...
    crs:         +init=epsg:4326
    res:         (0.0002694945852365922, 0.00026949458523998555)
    is_tiled:    0
    nodatavals:  (-3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38,...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are going to the data and try to create a spatial visualization. xarray has inbuilt plotting capabilities to visualize both temporal and spatial data using matplotlib. In my example I first alter the temporal dimension to have the correct time axis. I furthermore convert my xarray DataArray to a DataSet (&lt;a href=&#34;http://xarray.pydata.org/en/stable/data-structures.html&#34; target=&#34;_blank&#34;&gt;more here on the difference &lt;/a&gt;) and also filter the array to encorporate only positive values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a series with years
dates = pd.date_range(&#39;1984-01-01&#39;, periods=34,freq = &amp;quot;A&amp;quot;).year
ds.coords[&amp;quot;year&amp;quot;] = dates # Reset dates
ds = ds.to_dataset(&amp;quot;EVI&amp;quot;) # Convert to dataset
# Create a subset to run calcution on and correct scale + filter
img = ds.pipe(lambda x: x * 0.0001)
img = img.where(img &amp;gt;= 0)  
# Select the last item of year and plot it
img.isel(year=33).EVI.plot(robust=True,cmap=&#39;viridis&#39;)
# Finally plot a test year
plt.title(&#39;Annual EVI in year %s&#39; % (&#39;2017&#39;) )
plt.ylabel(&#39;Latitude&#39;)
plt.xlabel(&#39;Longitude&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/posts/AnnualEVI.png&#34; alt=&#34;Annual EVI as quantified by Landsat&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;basic-and-applied-calculations&#34;&gt;Basic and applied calculations&lt;/h3&gt;

&lt;p&gt;Next let us conduct some spatial-temporal analyses. &lt;br /&gt;
The xarray created above can in principle easily be summarized as a whole or on the pixel level. Both can be easily achieved by grouping observations on the pixel level.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds2.EVI.groupby(&#39;year&#39;).std().plot()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/posts/StdEVI.png&#34; alt=&#34;Standard deviation as time series&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Equally if we want to quantify the standard deviation across all 34 years in the stack&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds2.EVI.groupby(&#39;Longitude&#39;,&#39;Latitude&#39;).std(&#39;year&#39;).plot()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./img/posts/StdEVI_spatial.png&#34; alt=&#34;Standard deviation per pixel&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But what if want to apply a defined method over each pixel in given xarray?
This is again possible, but a little bit more complicated.
First we need to define a function that performs the requested analysis and returns a single value. In my example I want to calculate &lt;a href=&#34;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&#34; target=&#34;_blank&#34;&gt;Kendall&amp;rsquo;s rank correlation test&lt;/a&gt; for each time series over all pixels. Remember how we imported the &lt;strong&gt;scipy&lt;/strong&gt; stats package at the start of this post? To save time and harddrive space, we simply want to know for this simple example whether EVI is &amp;ldquo;significantly&amp;rdquo; ($p = 0.05$) increasing over the whole time series.
So let us build a function:
&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def k_cor(x,y, pthres = 0.05, direction = True):
    &amp;quot;&amp;quot;&amp;quot;
    Uses the scipy stats module to calculate a Kendall correlation test
    :x vector: Input pixel vector to run tests on
    :y vector: The date input vector
    :pthres: Significance of the underlying test
    :direction: output only direction as output (-1 &amp;amp; 1)
    &amp;quot;&amp;quot;&amp;quot;
    # Check NA values
    co = np.count_nonzero(~np.isnan(x))
    if co &amp;lt; 4: # If fewer than 4 observations return -9999
        return -9999
    # Run the kendalltau test
    tau, p_value = stats.kendalltau(x, y)

    # Criterium to return results in case of Significance
    if p_value &amp;lt; pthres:
        # Check direction
        if direction:
            if tau &amp;lt; 0:
                return -1
            elif tau &amp;gt; 0:
                return 1
        else:
            return tau
    else:
      return 0  

# The function we are going to use for applying our kendal test per pixel
def kendall_correlation(x,y,dim=&#39;year&#39;):
    # x = Pixel value, y = a vector containing the date, dim == dimension
    return xr.apply_ufunc(
        k_cor, x , y,
        input_core_dims=[[dim], [dim]],
        vectorize=True, # !Important!
        output_dtypes=[int]
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we execute our new function and to do so we first need to create a new DataArray that contains the y variable (date in our example). Afterwards we can apply our function on the loaded xarray.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = xr.DataArray(np.arange(len(ds2[&#39;year&#39;]))+1, dims=&#39;year&#39;,
                 coords={&#39;year&#39;: ds2[&#39;year&#39;]})  
r = kendall_correlation(ds2, x,&#39;year&#39;)                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This whole computation took us roughly 1min and 54 seconds on my Lenovo laptop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./img/posts/kcorEVI.png&#34; alt=&#34;Correlation test&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-speed-ups&#34;&gt;Further speed ups&lt;/h3&gt;

&lt;p&gt;In many cases an execution on a local computer hardly makes sense and is time inefficient. Particularly if your data is too large to fit into memory. This is usually the case with earth-observation data, which can easily become larger than 10GB++ .&lt;/p&gt;

&lt;p&gt;Luckily xarray supports parallel execution via the optional Dask &lt;a href=&#34;http://xarray.pydata.org/en/stable/dask.html&#34; target=&#34;_blank&#34;&gt;integration&lt;/a&gt;. So what is dask? Compared to standard numpy array calculations it supports lazy evaluation of any supplied code. This means that your code is only executed on the dataset as soon as you tell dask+xarray to do so (via the &lt;strong&gt;compute()&lt;/strong&gt; function ). Dask divides your array into many small pieces, so called chunks, each of which is presumed to be small enough to fit into memory. Chunking your data enables better parallelization which can easily be scaled up over multiple CPU cores or entire clusters of machines.
&lt;img src=&#34;./img/posts/grid_search_schedule.gif&#34; alt=&#34;Dask distributed scheduler&#34; /&gt;
&lt;br /&gt;
So how do we enable dask for your computation ? This is actually quite simple.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import dask stuff
import dask.array as da
from dask.diagnostics import ProgressBar

# Then while loading in your data specify that you want your data to be loaded as chunks
# A more optimal chunk size for your data can really speed up computation
# So ensure that your dataset is correctly chuncked.
ds = xr.open_rasterio(route,chunks={&#39;band&#39;: 34, &#39;x&#39;: 1000, &#39;y&#39;: 1000}).rename({&#39;x&#39;: &#39;Longitude&#39;, &#39;y&#39;: &#39;Latitude&#39;, &#39;band&#39;: &#39;year&#39; })                         

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice that your DataArray has now become a dask-array&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)&amp;gt;
dask.array&amp;lt;shape=(34, 1318, 1555), dtype=float32, chunksize=(34, 100, 100)&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you have to enable dask in your apply function and compute the result&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def kendall_correlation(x,y,dim=&#39;year&#39;):
    return xr.apply_ufunc(
        mk_cor, x , y,
        input_core_dims=[[dim], [dim]],
        vectorize=True,
        dask=&#39;parallelized&#39;, # Note the addition !!!
        output_dtypes=[int]
        )

# Make a little ProgressBar
with ProgressBar():
    # Until &#39;compute&#39; is run, no computation is executed
    r = kendall_correlation(ds2, x,&#39;year&#39;).compute()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running my computation with dask decreased the whole processing time down to 45s !&lt;/p&gt;

&lt;p&gt;But there is more we can do to further speed up the computation. By enabling &lt;a href=&#34;http://numba.pydata.org/&#34; target=&#34;_blank&#34;&gt;numba&lt;/a&gt; as just-in-time (JIT) byte compiler for our kendall correlation function we can squeeze a couple more seconds out. Numba compiles your python function into a byte-compiled snip that is a lot faster to execute.
One can enable the JIT compiler as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numba import jit #  Speedup for python functions

@jit(nogil=True) # Enable JIT compiler
def k_cor(x,y, pthres = 0.05, direction = True):
  ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note all this was run on a rather small dataset. The more data you have and the more computational intensive your analysis becomes, the more do the steps above improve your execution time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My system info&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linux Ubuntu - Codename bionic&lt;/li&gt;
&lt;li&gt;Python 3.6.5&lt;/li&gt;
&lt;li&gt;Numpy 1.13.3&lt;/li&gt;
&lt;li&gt;Xarray 0.10.2&lt;/li&gt;
&lt;li&gt;dask 0.16.0&lt;/li&gt;
&lt;li&gt;numba 0.34.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Full source code as gist&lt;/strong&gt;&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/Martin-Jung/d1946abc4310bc5a1cd50c53d1b029a7.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>The impact of economic recession on land changes in Greece</title>
      <link>http://martin-jung.github.io/project/greece/</link>
      <pubDate>Sun, 20 May 2018 20:24:48 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/project/greece/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Icarus</title>
      <link>http://martin-jung.github.io/project/icarus/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/project/icarus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New professional website</title>
      <link>http://martin-jung.github.io/2017/new-professional-website/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/2017/new-professional-website/</guid>
      <description>&lt;p&gt;Welcome to my new starting website. My name is Martin Jung and I consider myself an Environmental and data scientist.  &lt;a href=&#34;https://martin-jung.github.io/#about&#34; target=&#34;_blank&#34;&gt;Read here for more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have abandoned my previous online presence, the &lt;a href=&#34;http://conservationecology.wordpress.com/&#34; target=&#34;_blank&#34;&gt;Conservation ecology blog&lt;/a&gt; since quite a while. Mostly because I was too preoccupied working on my PhD. However I also disliked the inflexibility of wordpress layouts and the added advertising below each wordpress.com article. Therefore the restart of my professional website powered by hugo and the academic theme. So feel welcome if you are a previous reader of my blog, but notice that I will put more emphasis on examples and demonstrations on this website.&lt;/p&gt;

&lt;p&gt;Please do not hesitate to &lt;a href=&#34;https://martin-jung.github.io/#contact&#34; target=&#34;_blank&#34;&gt;contact me&lt;/a&gt;.&lt;br /&gt;
I&amp;rsquo;m particularly interested in scientific collaborations, consulting or data analyses offers. I also regularly give demonstrations and workshops on scientific data analysis (using R, python or QGIS), general data management (SQL database systems) and high performance computing. So please contact me if you think my services could be useful for your research group or company.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2017 Data analysis talk</title>
      <link>http://martin-jung.github.io/talk/2017_dacmtalk/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/talk/2017_dacmtalk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Birds in the matrix: the role of agriculture in avian conservation in the Taita Hills, Kenya</title>
      <link>http://martin-jung.github.io/publication/2017_birdsinthematrixafrica/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2017_birdsinthematrixafrica/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Remotely sensed forest monitoring in Bialowieza forest</title>
      <link>http://martin-jung.github.io/project/bialowieza/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/project/bialowieza/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Local factors mediate the response of biodiversity to land use on two African mountains</title>
      <link>http://martin-jung.github.io/publication/2017_mscthesisanimalconservation/</link>
      <pubDate>Wed, 28 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2017_mscthesisanimalconservation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The database of the PREDICTS (Projecting Responses of Ecological Diversity In Changing Terrestrial Systems) project</title>
      <link>http://martin-jung.github.io/publication/2016_predicts-database-paper2/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2016_predicts-database-paper2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying global forest fragmentation</title>
      <link>http://martin-jung.github.io/project/forestfrag/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/project/forestfrag/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Has land use pushed terrestrial biodiversity beyond the planetary boundary? A global assessment</title>
      <link>http://martin-jung.github.io/publication/2016_biodiversityplanetaryboundary/</link>
      <pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2016_biodiversityplanetaryboundary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Can we set a global threshold age to define mature forests?</title>
      <link>http://martin-jung.github.io/publication/2016_matureforestspeerj/</link>
      <pubDate>Thu, 04 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2016_matureforestspeerj/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
