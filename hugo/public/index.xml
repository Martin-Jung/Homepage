<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Martin Jung - Environmental and data scientist on Martin Jung - Environmental and data scientist</title>
    <link>http://martin-jung.github.io/</link>
    <description>Recent content in Martin Jung - Environmental and data scientist on Martin Jung - Environmental and data scientist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>Martin Jung 2017-2018 &amp;bull; &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;i class=&#34;fa fa-cc&#34; aria-hidden=&#34;true&#34;&gt;&lt;/i&gt;&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Testing multilevel Bayesian models with ordered categorical predictors</title>
      <link>http://martin-jung.github.io/post/2018_bayesianmodelsmonoticpredictors/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_bayesianmodelsmonoticpredictors/</guid>
      <description>&lt;p&gt;Why argue for Bayesian models? Most researchers and data scientists have specific  - domain - knowledge about the subject they analyse data for. In a Bayesian analysis framework this knowledge can be refereed to as &lt;em&gt;Prior&lt;/em&gt; and the effect und uncertainty surrounding this. Most standard analytical tools do not account for this information. In fact &lt;strong&gt;every&lt;/strong&gt; statistical tool makes some kind of assumptions about your data. Computers do not now &lt;em&gt;per se&lt;/em&gt; how your data looks or what limits it. By not providing this kind of information, the algorithm essentially has to guess or make uninformed assumptions, which can be quite unrealistic in applied settings.&lt;/p&gt;

&lt;p&gt;Or as Richard McElreath puts it:
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Still seeing researchers rejecting Bayes bc priors make them uncomfortable.&lt;br&gt;&lt;br&gt;If you don&amp;#39;t like priors, then don&amp;#39;t use Bayes. But you must do something to control overfitting. And that something will act a lot like a prior.  &lt;a href=&#34;https://t.co/FAZdcDS4B7&#34;&gt;https://t.co/FAZdcDS4B7&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard McElreath (@rlmcelreath) &lt;a href=&#34;https://twitter.com/rlmcelreath/status/1032318199713460224?ref_src=twsrc%5Etfw&#34;&gt;August 22, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Here is an example using data from my &lt;a href=&#34;http://martin-jung.github.io/project/phd/&#34; target=&#34;_blank&#34;&gt;PhD&lt;/a&gt;. In many of my PhD chapters I work with the &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.2579&#34; target=&#34;_blank&#34;&gt;PREDICTS database&lt;/a&gt;, a global database of local biodiversity records. A particular feature of the PREDICTS data collection is that the sites of biodiversity sampling are &amp;ldquo;re-categorized&amp;rdquo; into land-use categories such as &lt;em&gt;Primary vegetation&lt;/em&gt; or &lt;em&gt;Pasture&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A number of high-profile papers have been produced using PREDICTS data such as the one by my old supervisor, who used Hierarchical linear mixed effects models to estimate the average difference in species richness with land use globally
&lt;a href=&#34;http://discovery.ucl.ac.uk/1464937/1/Newbold%20etal%202015%20Nature%20Submitted.pdf&#34; target=&#34;_blank&#34;&gt;(Newbold et al. 2015)&lt;/a&gt;. Have a look at Newbold &lt;em&gt;et al.&lt;/em&gt; for more information about the study design and methods.&lt;/p&gt;

&lt;p&gt;The interesting question here is:
&lt;strong&gt;Should we consider land use as a monotonic gradient?&lt;/strong&gt; Or in other words: Do we expect that the difference between all categories other than the intercept (Primary vegetation) is negative, so a loss in species richness?
&lt;br&gt;
Another information that we can include is the expected difference or slope of local species loss. For instance we also can&amp;rsquo;t have more than 100 % loss of species richness for any site, so we have thus information about the - prior - distribution of the expected slope&lt;/p&gt;

&lt;p&gt;Most PREDICTS data has been released and is openly available through the &lt;a href=&#34;http://data.nhm.ac.uk/dataset/the-2016-release-of-the-predicts-database&#34; target=&#34;_blank&#34;&gt;National History Museum London&lt;/a&gt; and it should be possible to prepare a data frame similar to the one I used here.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
Lets first fit a roughly comparable model to Newbold et al. 2015:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Load packages and data for this project
library(tidyverse)
library(tidybayes)
library(brms)
library(lme4)
library(sjPlot)

# Load biodiversity site dataset
sites &amp;lt;- readRDS(&amp;quot;~/../PhD/Projects/P5_MagnitudeBreakpoints/PREDICTS_allsites.rds&amp;quot;)
# Merge primary vegetation to single category as reference
sites &amp;lt;- subset(sites,Predominant_habitat != &amp;quot;Cannot decide&amp;quot;)# Drop Cannot decide from the factors
sites$Predominant_habitat &amp;lt;- fct_collapse(sites$Predominant_habitat,&amp;quot;Primary vegetation&amp;quot; = c(&amp;quot;Primary forest&amp;quot;,&amp;quot;Primary non-forest&amp;quot;))

# Here all the categories we consider
sites$Predominant_habitat &amp;lt;- factor(sites$Predominant_habitat,
                                    levels = c(&amp;quot;Primary vegetation&amp;quot;,&amp;quot;Young secondary vegetation&amp;quot;,&amp;quot;Intermediate secondary vegetation&amp;quot;,&amp;quot;Mature secondary vegetation&amp;quot;,
                                               &amp;quot;Secondary vegetation (indeterminate age)&amp;quot;,&amp;quot;Plantation forest&amp;quot;,&amp;quot;Pasture&amp;quot;,&amp;quot;Cropland&amp;quot;,&amp;quot;Urban&amp;quot;),
                                    ordered = T)
sites$SS &amp;lt;- factor(sites$SS)

# A prediction container for later
nd &amp;lt;- data.frame(Predominant_habitat = factor(levels(sites$Predominant_habitat),levels = levels(sites$Predominant_habitat),ordered = T) ) # Prediction frame

# A function to transform all estimates relative to the intercept (that is primary vegetation)
relIntercept &amp;lt;- function(df){
  return( df %&amp;gt;% group_by(model) %&amp;gt;%
    mutate(fit = ( fit / fit[which(Predominant_habitat==&amp;quot;Primary vegetation&amp;quot;)])-1,
           fit.low = (fit.low / fit[which(Predominant_habitat==&amp;quot;Primary vegetation&amp;quot;)])-1,
           fit.high = (fit.high / fit[which(Predominant_habitat==&amp;quot;Primary vegetation&amp;quot;)])-1
    ) %&amp;gt;% ungroup()
  )
}

# First for lme4 using a &amp;quot;classic&amp;quot; predicts model and reproduce a model similar to Newbold et al. 2015
# As simple test we on use two random intercepts, the study and a spatial block within study
fit1 &amp;lt;- glmer(Species_richness ~ Predominant_habitat + (1|SS) + (1|SSB),
              data = sites,family = poisson(link = &amp;quot;log&amp;quot;))

# Save output
out &amp;lt;-
  data.frame(
  Predominant_habitat = nd$Predominant_habitat,
  fit = predict(fit1,newdata=nd,re.form=NA,type = &amp;quot;link&amp;quot;),
  fit.se = arm::se.fixef(fit1), # Get the standard error of the coefficients
  model = &amp;quot;lme4&amp;quot;
) %&amp;gt;% mutate(
  fit.low = exp((fit - fit.se*1.96)), # naively multiply the wald standard errors with 1.96
  fit.high = exp((fit + fit.se*1.96)),
  fit = exp(fit)
)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets create a similar model using a Bayesian framework. Here I will use the excellent &lt;em&gt;brms&lt;/em&gt; package to skip the lengthy procedure of writing Stan code myself. This is a &lt;strong&gt;very&lt;/strong&gt; simple model for demonstration purposes and can very likely quite improved.&lt;/p&gt;

&lt;p&gt;The Equation for the bayesian multi-level model :&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\text{Species richness}_i &amp;amp; \sim &amp;amp; \text{Poisson} (\mu_i) \\&lt;br /&gt;
\text{log} (\mu_i) &amp;amp; = &amp;amp; \alpha + \alpha_{\text{Study}_i} + \alpha_{\text{Spatial block}_i} + \beta \text{log} (\text{Land use}_i) \\&lt;br /&gt;
\beta &amp;amp; \sim &amp;amp; \text{Normal} (0, 1) \\&lt;br /&gt;
\alpha_{\text{Study,Spatial block}} &amp;amp; \sim &amp;amp; \text{Normal} (0, \sigma_{\text{Study,Spatial block}}) \\&lt;br /&gt;
\sigma_{\text{Study,Spatial block}} &amp;amp; \sim &amp;amp; \text{HalfCauchy} (0, 2) \\&lt;br /&gt;
\end{eqnarray}
$$
Priors for both intercepts are identical although one could argue that the spatial block should be specified differently given that it is nested within study.&lt;/p&gt;

&lt;p&gt;and can be fitted like thos:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit2 &amp;lt;- brm(Species_richness ~ Predominant_habitat + (1|SS) + (1|SSB),
            data = sites,family = poisson(link = &amp;quot;log&amp;quot;),
            prior = prior(normal(0,1), class = b) + # Normal distributed prior for beta
              prior(cauchy(0,2), class = sd), # Half cauchy prior for uncertainty
            chains = 2, cores = 6, iter = 2000) # Fit using 6 cores and 2000 iterations of 2 MCMC chains

# ... This takes a while.
# Made myself dinner

# How did we do?
# (showing only the intercept)
plot(fit2, ask =F)
&lt;/code&gt;&lt;/pre&gt;




  

&lt;figure&gt;

&lt;img src=&#34;http://martin-jung.github.io/img/posts/brmsMCMCchains.png&#34; alt=&#34;There are some irregularities in the iterations of the MCMC that do not look completely random yet and more interations might help. Or alternatively specifying a prior for the intercept :) &#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  &lt;p&gt;
    There are some irregularities in the iterations of the MCMC that do not look completely random yet and more interations might help. Or alternatively specifying a prior for the intercept :)
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;br&gt;
Lets compare the two models in their predicted impact of land use on species richness.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Get the prediction from the bayesian model
out &amp;lt;- bind_rows(out,
                 # Summarise from the fitted posterior values
                 fitted_draws(fit2,nd,re_formula = NA) %&amp;gt;% group_by(Predominant_habitat) %&amp;gt;% mean_qi(.value) %&amp;gt;%
                   select(Predominant_habitat,.value,.lower,.upper) %&amp;gt;% rename(fit = &amp;quot;.value&amp;quot;,fit.low = &amp;quot;.lower&amp;quot;, fit.high = &amp;quot;.upper&amp;quot;) %&amp;gt;%
                   mutate(fit.se = NA, model = &amp;quot;brms1&amp;quot;)
)

# Plot them both
ggplot(out %&amp;gt;% relIntercept(.),
       aes(x = fct_rev( Predominant_habitat ), y = fit, ymin = fit.low, ymax = fit.high, group = model, color =model)) +
  theme_default() + coord_flip()+
  geom_pointrange(position = position_dodge(.5)) + geom_hline(yintercept = 0, linetype = &amp;quot;dotted&amp;quot;)+
  scale_y_continuous(breaks = scales::pretty_breaks(5)) +
  scale_colour_brewer(palette = &amp;quot;Set1&amp;quot;) +
  labs(x=&amp;quot;&amp;quot;,y = &amp;quot;Predicted difference in Species richness&amp;quot;) + theme(axis.text.x = element_text(size = 15))

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;


  

&lt;figure&gt;

&lt;img src=&#34;http://martin-jung.github.io/img/posts/brmsModelcoef1.png&#34; alt=&#34;Ignore the error bars for now as they are not directly comparable (see below).&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;&lt;/h4&gt;
  &lt;p&gt;
    Ignore the error bars for now as they are not directly comparable (see below).
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;
It is noteworthy that the uncertainty for lme4 is the estimated standard error of the fitted object (&lt;strong&gt;!&lt;/strong&gt;), while for brms the estimation error is drawn only from the fitted values of the posterior.&lt;/p&gt;

&lt;p&gt;Still so far both models are quite similar and indicate comparable, mean losses of species richness as shown in the Newbold et al. 2015 Nature paper.
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Now here comes the interesting part. Paul Brückner, the author of the &lt;em&gt;brms&lt;/em&gt; R package has recently released a new &lt;a href=&#34;https://psyarxiv.com/9qkhj&#34; target=&#34;_blank&#34;&gt;preprint&lt;/a&gt; discussing the addition of monotonic effects to the &lt;em&gt;brms&lt;/em&gt; package. &lt;a href=&#34;https://en.wikipedia.org/wiki/Monotonic_function&#34; target=&#34;_blank&#34;&gt;Monotonic functions&lt;/a&gt; can be applied ordinal predictors for which a monotonic relationship (increasing, decreasing, concave, convex) is highly plausible.&lt;/p&gt;

&lt;p&gt;Fitting those kind of models in brms is now straight forward. We use the default uniform Dirichlet priors which according to Brückner generalize well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fit3 &amp;lt;- brm(Species_richness ~ mo(Predominant_habitat) + (1|SS) + (1|SSB),
            data = sites,family = poisson(link = &amp;quot;log&amp;quot;),
            prior = prior(normal(0,1), class = b) + prior(cauchy(0,2), class = sd),
            chains = 2, cores = 6, iter = 2000)

# Lets compare both models
waic(fit2,fit3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It does seem that the model without a monotonic effect is a better fit to the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;                WAIC     SE
fit2        170240.73 990.35
fit3        170533.55 997.99
fit2 - fit3   -292.82  99.12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How do the fitted effects compare ?
Quite differently. Notice how local species richness &lt;em&gt;monotonically&lt;/em&gt; declines with every further &amp;ldquo;level&amp;rdquo; of land use.&lt;/p&gt;




  

&lt;figure&gt;

&lt;img src=&#34;http://martin-jung.github.io/img/posts/brmsModelcoef2.png&#34; /&gt;


&lt;/figure&gt;

&lt;p&gt;Overall it does seem as if we have to reject the idea of &amp;ldquo;land-use gradients&amp;rdquo; at least if those gradients are quantified through categorical entities. One could argue that some - human altered - land-use categories can have higher number of species than some natural land use. Thinking for example of urban habitats for plants (lots of gardens, exotics and non-natives).&lt;/p&gt;

&lt;p&gt;In my PhD I try to find ways to capture land dynamics on a continuous unit scale rather than through categorical estimates. Look our for new results of this idea soon&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sessioninfo&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;R version 3.4.4 (2018-03-15)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.1 LTS

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C               LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
 [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8    LC_PAPER=en_GB.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] tidybayes_1.0.3 bindrcpp_0.2.2  sjPlot_2.6.1    mboost_2.9-1    stabs_0.6-3     lme4_1.1-19     Matrix_1.2-14  
 [8] brms_2.6.0      Rcpp_1.0.0      forcats_0.3.0   stringr_1.3.1   dplyr_0.7.8     purrr_0.2.5     readr_1.1.1    
[15] tidyr_0.8.2     tibble_1.4.2    ggplot2_3.1.0   tidyverse_1.2.1

loaded via a namespace (and not attached):
  [1] readxl_1.1.0              backports_1.1.2           plyr_1.8.4                igraph_1.2.2             
  [5] lazyeval_0.2.1            svUnit_0.7-12             TMB_1.7.15                splines_3.4.4            
  [9] crosstalk_1.0.0           TH.data_1.0-9             rstantools_1.5.1          inline_0.3.15            
 [13] digest_0.6.18             htmltools_0.3.6           rsconnect_0.8.11          lmerTest_3.0-1           
 [17] fansi_0.4.0               magrittr_1.5              modelr_0.1.2              matrixStats_0.54.0       
 [21] xts_0.11-2                sandwich_2.5-0            prettyunits_1.0.2         colorspace_1.3-2         
 [25] rvest_0.3.2               haven_1.1.2               callr_3.0.0               crayon_1.3.4             
 [29] jsonlite_1.5              libcoin_1.0-1             bindr_0.1.1               survival_2.42-3          
 [33] zoo_1.8-4                 glue_1.3.0                gtable_0.2.0              nnls_1.4                 
 [37] emmeans_1.3.0             sjstats_0.17.2            sjmisc_2.7.6              pkgbuild_1.0.2           
 [41] rstan_2.18.2              abind_1.4-5               scales_1.0.0              mvtnorm_1.0-8            
 [45] ggeffects_0.6.0           miniUI_0.1.1.1            xtable_1.8-3              HDInterval_0.2.0         
 [49] ggstance_0.3.1            foreign_0.8-70            Formula_1.2-3             stats4_3.4.4             
 [53] prediction_0.3.6          StanHeaders_2.18.0        DT_0.5                    htmlwidgets_1.3          
 [57] httr_1.3.1                threejs_0.3.1             arrayhelpers_1.0-20160527 RColorBrewer_1.1-2       
 [61] modeltools_0.2-22         pkgconfig_2.0.2           loo_2.0.0                 utf8_1.1.4               
 [65] labeling_0.3              tidyselect_0.2.5          rlang_0.3.0.1             reshape2_1.4.3           
 [69] later_0.7.5               munsell_0.5.0             cellranger_1.1.0          tools_3.4.4              
 [73] cli_1.0.1                 sjlabelled_1.0.14         broom_0.5.0               ggridges_0.5.1           
 [77] arm_1.10-1                yaml_2.2.0                processx_3.2.0            knitr_1.20               
 [81] coin_1.2-2                nlme_3.1-137              mime_0.6                  xml2_1.2.0               
 [85] debugme_1.1.0             compiler_3.4.4            bayesplot_1.6.0           shinythemes_1.1.2        
 [89] rstudioapi_0.8            stringi_1.2.4             ps_1.2.1                  Brobdingnag_1.2-6        
 [93] lattice_0.20-35           psych_1.8.10              nloptr_1.2.1              markdown_0.8             
 [97] shinyjs_1.0               stringdist_0.9.5.1        pillar_1.3.0              pwr_1.2-2                
[101] bridgesampling_0.6-0      estimability_1.3          data.table_1.11.8         httpuv_1.4.5             
[105] R6_2.3.0                  promises_1.0.1            gridExtra_2.3             codetools_0.2-15         
[109] colourpicker_1.0          MASS_7.3-50               gtools_3.8.1              assertthat_0.2.0         
[113] withr_2.1.2               shinystan_2.5.0           mnormt_1.5-5              multcomp_1.4-8           
[117] hms_0.4.2                 quadprog_1.5-5            grid_3.4.4                rpart_4.1-13             
[121] coda_0.19-2               glmmTMB_0.2.2.0           minqa_1.2.4               inum_1.0-0               
[125] snakecase_0.9.2           partykit_1.2-2            numDeriv_2016.8-1         shiny_1.2.0              
[129] lubridate_1.7.4           base64enc_0.1-3           dygraphs_1.1.1.6
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Local species assemblages are influenced more by past than current dissimilarities in photosynthetic activity</title>
      <link>http://martin-jung.github.io/publication/2018_jungpairwisedifferences/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2018_jungpairwisedifferences/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New PhD paper - Pairwise differences in photosynthetic activity</title>
      <link>http://martin-jung.github.io/post/2018_newphdpublication/</link>
      <pubDate>Mon, 03 Sep 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_newphdpublication/</guid>
      <description>&lt;p&gt;I can now happily announce that the first chapter of my PhD has been accepted and is going to be published in the Journal &lt;a href=&#34;https://onlinelibrary.wiley.com/journal/16000587&#34; target=&#34;_blank&#34;&gt;Ecography&lt;/a&gt;. In this piece I investigated whether pairwise compositional differences between species assemblages (so in the identity and amount of species found in a given place and time) can be explained by a dissimilarity in remotely-sensed vegetation index. &lt;br&gt;&lt;/p&gt;

&lt;p&gt;I furthermore found that past dissimilarities in photosynthetic activity improved the overall fit relative to current dissimilarities, therefore hinting at a &lt;em&gt;lag-effect&lt;/em&gt; that can be investigated via remote sensing. Also quite cool is that we use - to my knowledge for the first time - the same dissimilarity metric (the &lt;strong&gt;Bray-Curtis&lt;/strong&gt; index) to quantify both differences between species assemblages and entire remotely-sensed time series of photosynthetic activity.
Terminology and writeup took quite a while for this piece, so I am quite happy that it is finally out.
&lt;br /&gt;
&lt;br&gt;
You can find the abstract and links to paper etc. &lt;a href=&#34;http://martin-jung.github.io/publication/2018_jungpairwisedifferences/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running rstudio in the Google cloud - [2]</title>
      <link>http://martin-jung.github.io/post/2018_datascienceingooglecloud_nr2/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_datascienceingooglecloud_nr2/</guid>
      <description>

&lt;p&gt;In this new post I will go through my process of getting familiar with running &lt;em&gt;R&lt;/em&gt; in the Google cloud and the posting sort of follows my previous &lt;a href=&#34;http://martin-jung.github.io/post/2018_datascienceingooglecloud/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt; on getting started with the Google cloud. My dream setup would include to being able to switch seamless between running r code locally or in the cloud whenever I require more processing power. For instance similar &lt;a href=&#34;https://github.com/Azure/doAzureParallel&#34; target=&#34;_blank&#34;&gt;doAzureParallel&lt;/a&gt; package available for Microsoft Azure.
&lt;br&gt;
For Google cloud engine, there also exists a neat package called &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/index.html&#34; target=&#34;_blank&#34;&gt;googleComputeEngineR&lt;/a&gt;, that allows to easily setup a virtual machine and run code remotely.
So let&amp;rsquo;s setup the googleComputeEngineR package. As always, please note you alone (the dear reader) is responsible to keep track of your virtual machines in the cloud. If you do not stop them (i.e. shut them down), then this will &lt;span class=&#34;markup-quote&#34;&gt;cost you money!&lt;/span&gt;
&lt;hr&gt;
In order to use the &lt;strong&gt;googleComputeEngineR&lt;/strong&gt; package, we first need to create a credentials file. For my google cloud project and personal linux machine I have created such a file on my local system like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create the file
touch ~/.Renviron
echo &amp;quot;GCE_AUTH_FILE=\&amp;quot;~/wasserdampf.json\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
echo &amp;quot;GCE_DEFAULT_PROJECT_ID=\&amp;quot;wolke7-208420\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
echo &amp;quot;GCE_DEFAULT_ZONE=\&amp;quot;us-central1-a\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One also needs a service account auth key (here called wasserdampf.json). Find more information how to get such a key &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/installation-and-authentication.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.
Now for starters lets start R and install the &lt;strong&gt;googleComputeEngineR&lt;/strong&gt; package, then start up a virtual machine with Rstudio setup.&lt;/p&gt;

&lt;h2 id=&#34;run-a-rstudio-in-the-google-cloud&#34;&gt;Run a Rstudio in the google cloud&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;llibrary(googleAuthR)
library(googleComputeEngineR)
library(future)

# Start up a rstudio vm (or create if not already existing)
vm &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
             name = &amp;quot;rstudio&amp;quot;,
             username = &amp;quot;martin&amp;quot;, password = &amp;quot;wolkenwind&amp;quot;,
             predefined_type = &amp;quot;n1-standard-1&amp;quot; # Available machines via gce_list_machinetype()
)

# See if the vm exists
gce_list_instances()

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; ==Google Compute Engine Instance List==
&amp;gt;      name   machineType  status          zone     externalIP   creationTimestamp
&amp;gt; 1 rstudio n1-standard-1 RUNNING us-central1-a XX.XXX.XXX.XXX 2018-08-23 14:41:45
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The externalIP gives the ip through which rstudio server can be run in any webbrowser
&lt;img src=&#34;http://martin-jung.github.io/img/posts/GoogleCloud_Rstudio.png&#34; alt=&#34;Rstudio run in the google cloud&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Equally it is quite easy to control the VM via SSH directly in the browser and the &lt;strong&gt;googleComputeEngineR&lt;/strong&gt; package provides an easy function to open such a connection:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;gce_ssh_browser(vm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly ensure that you stop the VM(or delete it).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-R&#34;&gt;# Shut down the vm
gce_vm_stop(vm)

# Or delete the vm
gce_vm_delete(vm)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Google Earth Engine</title>
      <link>http://martin-jung.github.io/post/2018_googleearthengine-intro/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_googleearthengine-intro/</guid>
      <description>&lt;p&gt;In a previous post, I gave a brief introduction how to use &lt;a href=&#34;http://martin-jung.github.io/post/2018_datascienceingooglecloud/&#34; target=&#34;_blank&#34;&gt;google cloud compute&lt;/a&gt; to kickstart your cloud computing experience. While it is possible to run large spatial operations on google cloud compute, it is quite time-consuming to set up all the routines to load and process geospatial data. Luckily there is now a new platform (currently in beta-testing) called Google Earth Engine (GEE) described as &lt;a href=&#34;https://earthengine.google.com/&#34; target=&#34;_blank&#34;&gt;planetary scale platform for spatial analyses&lt;/a&gt;.
&lt;br /&gt;
&lt;br&gt;
GEE is (currently) free to use and users can sign up for a &lt;a href=&#34;https://earthengine.google.com/signup/&#34; target=&#34;_blank&#34;&gt;beta-testing&lt;/a&gt;. There are a &lt;a href=&#34;https://earthengine.google.com/datasets/&#34; target=&#34;_blank&#34;&gt;wide range of datasets&lt;/a&gt; already available within GEE, including all available Landsat, Sentinel and MODIS satellite data. GEE furthermore allows to pre-process these layers and the fantastatic google team also already pre-computed layers such as cloud masks or ingested fully radiometrically corrected layers. If the reader is interested in what is possible with GEE, have a look at the &lt;a href=&#34;https://earthengine.google.com/case_studies/&#34; target=&#34;_blank&#34;&gt;case studies&lt;/a&gt; on the GEE website.
&lt;br /&gt;
&lt;hr&gt;
In the rest of this post I will showcase some exemplary scripts I coded in &lt;strong&gt;Javascript&lt;/strong&gt; which, besides &lt;strong&gt;python&lt;/strong&gt;, is the primary way to access and pre-process data in GEE. I will mostly only comment on my script as there are extensive tutorials, videos and detailed API descriptions &lt;a href=&#34;https://developers.google.com/earth-engine/&#34; target=&#34;_blank&#34;&gt;available&lt;/a&gt; for GEE.
&lt;br /&gt;
Whenever I load outside data (such as ESRI shapefiles) into GEE, I usually convert them to a KML file and then load them as Google Fusion Table (search online how to do this). All the (java-)scripts below can be pasted into the &lt;a href=&#34;https://code.earthengine.google.com&#34; target=&#34;_blank&#34;&gt;GEE code console&lt;/a&gt;, but if any errors occur then usually because of missing permissions (you might not be able to access my Google Fusion Tables).
&lt;/p&gt;

&lt;p&gt;The following script quantifies the date of forest loss from the &lt;a href=&#34;https://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.5.html&#34; target=&#34;_blank&#34;&gt;Hansen forest cover dataset&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// Load the global Hansen forest dataset
var gfcImage = ee.Image(&amp;quot;UMD/hansen/global_forest_change_2017_v1_5&amp;quot;);
/*
@author Martin Jung - m.jung@sussex.ac.uk
Idea:
Get average date of forest loss within sampling extent
*/

// Coordinates and parameters
// (This is a fusiontable id containing my polygon shapefile)
var fullsites = ee.FeatureCollection(&#39;ft:1Oet2yGWvldNoVx8A6ZlCi75Ks_lrEG0dG9oD6k2j&#39;); // All sites

//////// Other Parameters //////////
var export_geometry = false; // export the geometry in the csv
var scale = 30; // Resolution over which image collection should be reduced, 30 m = native scale
var reduce = ee.Reducer.mean() // Reducer for image collection
// Also available max(), mean(), median(), min(), mode(), or(), product(), sum(), stdDev()
var what = &amp;quot;ForestLoss&amp;quot;; // Export name

// ################################################################### //
//                Function and main CODE starts here                   //
// ################################################################### //

// Create TimeBand
function createTimeBand(image) {
  return image.addBands(image.metadata(&#39;system:time_start&#39;));
}

// ---- //
// Select the band with loss year
var forestImage = gfcImage.select([&#39;lossyear&#39;]);
// Mask out pixels with no lossyear
var m = forestImage.gt(0);
forestImage = forestImage.mask(m);

// -------------------------------- //
// Reduce per polygon per polygon
var extracted = forestImage.reduceRegions(fullsites, reduce, scale);

Export.table.toDrive({
  collection: extracted.select([&amp;quot;.*&amp;quot;], null, export_geometry),
  description: &amp;quot;PREDICTSHansen_&amp;quot;+what,
  fileNamePrefix: &amp;quot;PREDICTSHansen_&amp;quot;+what,
  fileFormat: &#39;geoJSON&#39;
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br&gt;
Not too difficult, right?
&lt;br /&gt;
Here is another script that calculates a 95 % percentile composite of three months of EVI data calculated from all available Landsat surface reflectance images from 1984 to 2017. My script furthermore exclude all pixels that are clearly covered by water (using data from &lt;a href=&#34;https://www.nature.com/articles/nature20584&#34; target=&#34;_blank&#34;&gt;Pekel et al. 2016&lt;/a&gt; ) and mask out unsuitable images (too many clouds) as well as clouds and cloud-shadows. Finally the mean, composited EVI within a buffer was exported.
Full script below:
&lt;br&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// Images and shapefiles
// Including landsat Level 1 land-surface-reflectance bands
var l8_led = ee.ImageCollection(&amp;quot;LANDSAT/LC08/C01/T1_SR&amp;quot;),
    l4_led = ee.ImageCollection(&amp;quot;LANDSAT/LT04/C01/T1_SR&amp;quot;),
    l5_led = ee.ImageCollection(&amp;quot;LANDSAT/LT05/C01/T1_SR&amp;quot;),
    l7_led = ee.ImageCollection(&amp;quot;LANDSAT/LE07/C01/T1_SR&amp;quot;),
    gsw = ee.Image(&amp;quot;JRC/GSW1_0/GlobalSurfaceWater&amp;quot;), // Global Surface water dataset
    usgadm = ee.FeatureCollection(&amp;quot;ft:1kA8n0e3lvUVuWJSixVf5WaPyeqskp8kZg9zU9TBX&amp;quot;),
    bbs_union = ee.FeatureCollection(&amp;quot;ft:1hqCRJam4b3niJZgRmJkOOMEe_qTwM77gZ-TyP4pe&amp;quot;), // Shapefiles I use my analyses. They contain polygons
    bbs = ee.FeatureCollection(&amp;quot;ft:1gUxInqbJAG-kcH6bn6PioNnZ6G7fZOBIIaEADnIO&amp;quot;);
/*
author: Martin Jung - 2018 (m.jung@sussex.ac.uk)
Idea:
Calculate avg overall EVI for all routes.
*/
// Parameters
var scale = 30; // Resolution over which image collection should be reduced, 30m = native scale
var precision = 10000; // Rounding precision
var cloudlimit = 80; // Should images with that many clouds be kept ? In order to reduce effect on non-filtered shades
var reduce = ee.Reducer.percentile([95]) ; // How should composites be reduced?
var startmonth = 3; // start Month when to use images from
var endmonth = 6; // end month
var startday = 20; // start day
var endday = 20; // end day
var fname = &amp;quot;EVIstack&amp;quot;; // FileNamedDescription
var what = &#39;evi&#39;; // Name of resulting spectral index band

// Expressions
var f_evi = &#39;2.5 * ((nir - red) / (nir + 2.4 * red + 1))&#39;; // EVI2 formula (two-band version)

// ################################################################### //
//                Function and main CODE starts here                   //
// ################################################################### //

// Function to mask excess EVI values defined as &amp;gt; 1 and &amp;lt; 0
var maskExcess = function(image) {
    var hi = image.lte(1);
    var lo = image.gte(0);
    var masked = image.mask(hi.and(lo));
    return image.mask(masked);
  };

// Function to remove clouds - expects the new SR data to have a cfmask layer
// 126122017 - Adapted to work with LT1
var maskclouds = function(scene) {
  // The &amp;quot;pixel_qa&amp;quot; band has various flags encoded in
  // different bits.  We extract some of them as individual mask bands.
  // Note: Cloud masking information is present in &amp;quot;pixel_qa&amp;quot;
  // pixel_qa Bit 1: Clear pixel indicator.
  // pixel_qa Bit 2: Water indicator.
  // pixel_qa Bit 3: Cloud shadow indicator.
  // pixel_qa Bit 5: Cloud indicator.
  // pixel_qa Bits 6-7: Cloud confidence.
  // Fill = https://explorer.earthengine.google.com/#detail/LANDSAT%2FLE07%2FC01%2FT1_SR
  var clear = scene.select(&#39;pixel_qa&#39;).bitwiseAnd(2).neq(0);
  clear = scene.updateMask(clear);
  return(clear);
};

// Water mask all pixels with over 90 % water occurence
var water_mask = gsw.select(&#39;occurrence&#39;).gt(90).unmask(0);
var watermask = function(image){
  var masked = water_mask.eq(0);
  return image.mask(masked);
};

// Create TimeBand
function createTimeBand(image) {
  return image.addBands(image.metadata(&#39;system:time_start&#39;));
}

// Filter out those bands with no images and create and empty image where there is none
// (This function should not be necessary as the area I investigate has full availability of Landsat 4-8)
var conditional = function(image) {
  return ee.Algorithms.If(ee.Number(image.get(&#39;num_elements&#39;)).gt(0),
                          image,
                          ee.Image(0).toDouble()
                          .set(&#39;system:time_start&#39;,image.get(&#39;system:time_start&#39;))
                          .rename(what));
};

// VegIndex calculator. Calculate the EVI index (two-band versiob)
function calcIndex(image){
  var evi = image.expression(
      f_evi,
        {
          red: image.select(&#39;red&#39;).multiply(0.0001),    // 620-670nm, RED
          nir: image.select(&#39;nir&#39;).multiply(0.0001)    // 841-876nm, NIR
        });
    // Rename that band to something appropriate
    var dimage = ee.Date(ee.Number(image.get(&#39;system:time_start&#39;))).format();
    return evi.select([0], [what]).set({&#39;datef&#39;: dimage,&#39;system:time_start&#39;: ee.Number(image.get(&#39;system:time_start&#39;))});
}

// ----------------------------------------- //
// Further PROCESSING CODE STARTS BELOW              //
// ----------------------------------------- //

// Load the polygon shapefiles for my analysis and use a bounding box to clip all outputs
var bbs_filter = bbs_union;
var bbox = bbs_filter.geometry().bounds();
Map.addLayer(bbs_filter);Map.addLayer(bbox) // Add to map and center to it


  // Filter the layers and set bounds
  // get the LC8 collection
  var L8 = l8_led
     .filterBounds(bbox) // filter all Landsat images by bound
     .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling
     .filterMetadata(&#39;CLOUD_COVER_LAND&#39;,&#39;less_than&#39;,cloudlimit) // Ignore images with too many clouds
     .map(maskclouds) // mask clouds and cloud-shadows from the image
     .map(watermask) // mask out water
     .map(createTimeBand); // add a time band

  // get the LE7 collection
  var L7 = l7_led
     .filterBounds(bbox)
     .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling
     .filterMetadata(&#39;CLOUD_COVER_LAND&#39;,&#39;less_than&#39;,cloudlimit)
     .map(maskclouds)
     .map(watermask)
     .map(createTimeBand);

  // get the LE5 collection
  var L5 = l5_led
     .filterBounds(bbox)
     .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling
     .filterMetadata(&#39;CLOUD_COVER_LAND&#39;,&#39;less_than&#39;,cloudlimit)
     .map(maskclouds)
     .map(watermask)
     .map(createTimeBand);

  // get the LE5 collection
  var L4 = l4_led
     .filterBounds(bbox)
     .filterDate(ee.Date.fromYMD(1984,startmonth,startday), ee.Date.fromYMD(2017,endmonth,endday)) // Filter to up to latest sampling
     .filterMetadata(&#39;CLOUD_COVER_LAND&#39;,&#39;less_than&#39;,cloudlimit)
     .map(maskclouds)
     .map(watermask)
     .map(createTimeBand);

  // Rename bands for all (note that band number change between Landsat satellites)
  var L4 = L4.map(function(image){
    return image.select(
      [&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;,&#39;B5&#39;,&#39;B7&#39;],
      [&#39;blue&#39;,&#39;green&#39;,&#39;red&#39;,&#39;nir&#39;,&#39;swir1&#39;,&#39;swir2&#39;]
      );
  });
  var L5 = L5.map(function(image){
    return image.select(
      [&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;,&#39;B5&#39;,&#39;B7&#39;],
      [&#39;blue&#39;,&#39;green&#39;,&#39;red&#39;,&#39;nir&#39;,&#39;swir1&#39;,&#39;swir2&#39;]
      );
  });
  var L7 = L7.map(function(image){
    return image.select(
      [&#39;B1&#39;,&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;,&#39;B5&#39;,&#39;B7&#39;],
      [&#39;blue&#39;,&#39;green&#39;,&#39;red&#39;,&#39;nir&#39;,&#39;swir1&#39;,&#39;swir2&#39;]
      );
  });
  var L8 = L8.map(function(image){
    return image.select(
      [&#39;B2&#39;,&#39;B3&#39;,&#39;B4&#39;,&#39;B5&#39;,&#39;B6&#39;,&#39;B7&#39;],
      [&#39;blue&#39;,&#39;green&#39;,&#39;red&#39;,&#39;nir&#39;,&#39;swir1&#39;,&#39;swir2&#39;]
      );
  });

  // Merge the collections
  // this collection is sorted by time
  var Collection = ee.ImageCollection(L8.merge(L7))
                        .sort(&#39;system:time_start&#39;,true);
  Collection = ee.ImageCollection(Collection.merge(L5))
                        .sort(&#39;system:time_start&#39;,true);
  Collection = ee.ImageCollection(Collection.merge(L4))
                        .sort(&#39;system:time_start&#39;,true);

  // Calculate an vegetation index on full collection
  Collection = Collection.map( calcIndex );
  // --------------------------------------------------- //
  // Mask out pixels with excess values (sensor errors)
  Collection = Collection.map( maskExcess );

  // Clip to feature collection geometry
  Collection = Collection.map(function(i){return i.clip(bbs_filter);});    

  // Reduce the time series of images into a single image
  var img = Collection.reduce(reduce);
  // --------------------------------- //

  // Run function to calculate the mean per polygon
  var extract = function(img,bbs){
    var extracted = img.reduceRegions(bbs, ee.Reducer.mean(), scale);
    return extracted;
  };

  // The extracted results
  var results = extract(img, bbs);

  // Export the output
  Export.table.toDrive({
      collection: results,
      folder: &#39;CropscapeTest&#39;, // Folder name in google drive
      description: &#39;Annual_&#39; + fname + &amp;quot;_AvgEVI&amp;quot; + &#39;_&#39;+fname,
      fileNamePrefix : &#39;Annual_&#39; + fname + &amp;quot;_AvgEVI&amp;quot; + &#39;_&#39;+fname,
      fileFormat: &#39;geoJSON&#39;
  });

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hope these examples were helpful. I might post more of my code-examples at a later point.&lt;/p&gt;

&lt;p&gt;Cheers, &lt;br /&gt;
Martin&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD research image prize</title>
      <link>http://martin-jung.github.io/post/2018-phd-poster-prize/</link>
      <pubDate>Sat, 30 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018-phd-poster-prize/</guid>
      <description>

&lt;p&gt;I won a prize :-) &lt;br /&gt;
In this years &lt;a href=&#34;http://www.sussex.ac.uk/internal/doctoralschool/newsandevents/festival&#34; target=&#34;_blank&#34;&gt;Festival of Doctoral research&lt;/a&gt; at the University of Sussex I won the runner-up prize for the best image describing one&amp;rsquo;s doctoral research. The task was to submit a single image and 300 word abstract that visualizes and describes the goals of a persons doctoral research.&lt;/p&gt;

&lt;p&gt;Picture seen above (&lt;a href=&#34;http://martin-jung.github.io/img/posts/WideFormat2.jpg&#34; target=&#34;_blank&#34;&gt;click here for larger version&lt;/a&gt;). Here is the accompanying description:&lt;/p&gt;

&lt;h3 id=&#34;title-going-back-in-time-with-satellites-to-assist-biodiversity-conservation&#34;&gt;Title: Going back in time with satellites to assist biodiversity conservation&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;In my PhD I investigate how past land changes continue to affect local biodiversity. The above pictures were created from multiple satellite images (courtesy of the U.S. Geological Survey) and depict parts of the Roneam Daun Sam Wildlife sanctuary near the border (red line) of Thailand and Cambodia. In 2003 the sanctuary was reported to still have over 39 961 hectares of intact forest. Yet, on the 22nd of February 2018 the sanctuary has been officially dissolved by Cambodian royal degree owing to centuries of illegal timber harvesting. While the sanctuary has been ineffective in protecting its forest, it is unclear how strongly biodiversity in the area was and continues to be affected by these land changes. In my research I apply statistical models to link both satellite and local biodiversity survey data in order to quantify the magnitude of these changes in Cambodia and other areas globally.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Data science in the Google cloud - [1]</title>
      <link>http://martin-jung.github.io/post/2018_datascienceingooglecloud/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018_datascienceingooglecloud/</guid>
      <description>&lt;p&gt;Anyone analysing &lt;strong&gt;big data&lt;/strong&gt; (buzzword, here refereed to as data too big to load into memory) soon will come to the realization that processing such data requires a lot of computational resources. During my PhD I mainly worked with the local high-performance-computer (HPC) at the University of Sussex. A couple of years into my PhD and I increasingly realized that our little HPC suffers from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Tragedy_of_the_commons&#34; target=&#34;_blank&#34;&gt;tragedy of the commons&lt;/a&gt; with more and more people requesting computation time on a few available nodes.  That and also the tendency to have limited flexibility for running customized code (no root access, outdated modules and libraries, little space on the home drive to set up virtual environments, etc. &amp;hellip;) has made me quite frustrated and willing to switch to the &amp;ldquo;Cloud&amp;rdquo; for accessing computing resources.
&lt;/p&gt;

&lt;p&gt;Cloud computing these days is well established, but mainly concentrated in the hands of three leading US firms. As far as I am aware one basically has to choose between Amazon AWS, Microsoft Azure and Google Cloud programs. Each have their own benefits and I leave it to the reader to search elsewhere for information on which one to chose.
&lt;/p&gt;

&lt;p&gt;I picked the &lt;a href=&#34;https://cloud.google.com/free-trial/&#34; target=&#34;_blank&#34;&gt;Google cloud free trial offer&lt;/a&gt; partly because of the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;They have a 300$ give away. (I think Microsoft and Amazon offer sth. similar though)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The free trial period lasts 12 months after which it runs out without incurring further cost. Furthermore there will remain a &lt;a href=&#34;https://cloud.google.com/free/&#34; target=&#34;_blank&#34;&gt;free-use contingent&lt;/a&gt; which can be exhausted for free. You fire up some use time on a f1-micro VM for instance.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I am increasingly using &lt;a href=&#34;https://earthengine.google.com/&#34; target=&#34;_blank&#34;&gt;Google&amp;rsquo;s Earth Engine platform&lt;/a&gt; and plan to use Google cloud storage to enhance my workflow.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Private 1GB Git hosting (now especially useful since Competitor Microsoft has acquired Github )&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That being said, I have also heard great things about AWS and Azure as well and might try them out at a later point as well.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;So here is how I started. My goal was to first get familiar with computing in the cloud and try to install some standard tools. Therefore
First I fired up a micro instance &lt;strong&gt;V&lt;/strong&gt;irtual &lt;strong&gt;M&lt;/strong&gt;achine (which, in the google cloud, you can run over 700h each month for free).
&lt;img src=&#34;http://martin-jung.github.io/img/posts/GoogleCloudInstance.png&#34; alt=&#34;Micro instance in Google cloud &#34; /&gt;
On the SSH button you have the opportunity to directly log into your cloud instance in the browser or in another ssh-client of you choosing.
Each VM can be selected and also started / stopped or completly reseted in this screen as well (also via the &lt;strong&gt;&lt;em&gt;&amp;rdquo;&amp;hellip;&amp;rdquo;&lt;/em&gt;&lt;/strong&gt; button!)
&lt;br /&gt;
I&amp;rsquo;m going to install some basic data-science tools.
Here is the entire thing as bash-script to be executed on the next, bigger, VM in a later stage ;-)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# First lets install some necessary libraries
sudo apt-get -y install bzip2
sudo apt-get -y install screen

# Make a update and upgrade all, then clean up
sudo apt-get update
sudo apt-get -y upgrade
sudo apt-get -y autoremove

# Make download folder
mkdir downloads
cd downloads
# Download anaconda
wget https://repo.continuum.io/archive/Anaconda2-5.2.0-Linux-x86_64.sh
# Install in the background (accept and updating any previous installations)
bash Anaconda2-5.2.0-Linux-x86_64.sh -b -u -p $HOME/anaconda2
echo &amp;quot;export PATH=\&amp;quot;~/anaconda2/bin:$PATH\&amp;quot;&amp;quot; &amp;gt;&amp;gt; ~/.bashrc
# Reload conf
source ~/.bashrc

# Install R
# Add debian stretch repo and key, then install
echo &amp;quot;deb http://cran.rstudio.com/bin/linux/debian stretch-cran35/&amp;quot; | sudo tee -a /etc/apt/sources.list
sudo apt-key adv --keyserver keys.gnupg.net --recv-key &#39;E19F5F87128899B192B1A2C2AD5F960A256A04AF&#39;
sudo apt-get update
sudo apt-get install -y r-base r-base-core r-base-dev
sudo apt-get install -y libatlas3-base

# Also install rstudio keyserver
sudo apt-get -y install psmisc libssl-dev libcurl4-openssl-dev libssh2-1-dev
wget https://download2.rstudio.org/rstudio-server-stretch-1.1.453-amd64.deb
sudo dpkg -i rstudio-server-stretch-1.1.453-amd64.deb

# Also install julia for later
sudo apt-get -y install julia

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note to myself&lt;/strong&gt;: For the future it might be easier to configure an analysis-ready docker image. Sth. to do for later&amp;hellip;
&lt;/p&gt;

&lt;p&gt;Now we create a new configuration for a jupyter notebook and start it on the vm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create config
jupyter notebook --generate-config

# Add this to the configure
echo &amp;quot;c = get_config()&amp;quot; &amp;gt;&amp;gt; ~/.jupyter/jupyter_notebook_config.py
echo &amp;quot;c.NotebookApp.ip = &#39;*&#39;&amp;quot; &amp;gt;&amp;gt; ~/.jupyter/jupyter_notebook_config.py
echo &amp;quot;c.NotebookApp.open_browser = False&amp;quot; &amp;gt;&amp;gt; ~/.jupyter/jupyter_notebook_config.py
echo &amp;quot;c.NotebookApp.port = 8177&amp;quot; &amp;gt;&amp;gt; ~/.jupyter/jupyter_notebook_config.py

# Set a password
jupyter notebook password

# Start up
jupyter-notebook --no-browser --port=8177

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The jupyter notebook can now be theoretically viewed in a browser. However we have to get access to the Google cloud intranet first. For this we will use the &lt;a href=&#34;https://cloud.google.com/sdk/&#34; target=&#34;_blank&#34;&gt;google cloud SDK&lt;/a&gt;, which you need to install on your local computer as well.&lt;/p&gt;

&lt;p&gt;Then execute for the google cloud sdk:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# After installation: auth
gcloud init

# The open a SSH tunnel. For me that is:
gcloud compute ssh  --zone=us-central1-c --ssh-flag=&amp;quot;-D&amp;quot; --ssh-flag=&amp;quot;8177&amp;quot; --ssh-flag=&amp;quot;-N&amp;quot; --ssh-flag=&amp;quot;-n&amp;quot; wolkentest
# If you have never done before, you will need to create a public/private ssh key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that you have created a SSH tunnel you can just open your local browser (ie. Chrome or similar) and navigate towards &lt;a href=&#34;localhost:8177&#34; target=&#34;_blank&#34;&gt;localhost:8177&lt;/a&gt; and you should see your jupyter notebook. Happy computing!
&lt;img src=&#34;http://martin-jung.github.io/img/posts/GoogleCloudJupyterRunning.png&#34; alt=&#34;Jupyter running through an SSH tunnel&#34; /&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;At the end, ensure that the VM is turned off, otherwise it will create ongoing costs!&lt;/p&gt;

&lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust correlations with xarray and dask</title>
      <link>http://martin-jung.github.io/post/2018-xarrayregression/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2018-xarrayregression/</guid>
      <description>

&lt;p&gt;I perform most of my analyses using either &lt;strong&gt;R&lt;/strong&gt; or standalone &lt;strong&gt;GDAL&lt;/strong&gt; tools simply because of their general convenience and ease of use. Standard spatial analysis functions and tools are in my opinion still more readily available in R and most R packages are quite mature and well designed ( &lt;a href=&#34;http://www.rspatial.org/&#34; target=&#34;_blank&#34;&gt;but see the readme&lt;/a&gt; ). Nevertheless &lt;strong&gt;python&lt;/strong&gt; has caught up and a number of really helpful python modules for spatial analyses have been released to this date. I have always loved coding in python since I developed my &lt;a href=&#34;https://github.com/Martin-Jung/LecoS&#34; target=&#34;_blank&#34;&gt;LecoS plugin&lt;/a&gt;, especially because of it&amp;rsquo;s processing speed (doing any computation on numpy arrays is quite fun) and clean synthax. In this post I will demonstrate how to make use of the new (?) &lt;a href=&#34;http://xarray.pydata.org&#34; target=&#34;_blank&#34;&gt;xarray&lt;/a&gt; (previously xray) python module to load remotely-sensed data and run &amp;ldquo;pixel-wise&amp;rdquo; correlation tests on them.&lt;/p&gt;

&lt;p&gt;I cannot release the shown spatial data, however any &amp;lsquo;stack&amp;rsquo; of remotely-sensed data or multiple satellite images will do as long as you can assign a time dimension/axis to the array.&lt;/p&gt;

&lt;p&gt;All python modules should be easible downloadable via the package manager of your choice such as wheel, pip and conda &amp;amp; co.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;So what is &lt;strong&gt;xarray&lt;/strong&gt;? In short it is a python package that enables explicit computation on labeled, multi-dimensional arrays, such as those commonly obtained from geophysical models or repeated satellite images. Think of it as a convenience wrapper that combines the best of &lt;a href=&#34;https://pandas.pydata.org/&#34; target=&#34;_blank&#34;&gt;pandas&lt;/a&gt; data structures with numpy&amp;rsquo;s array functionalities. It has been developed specifically with geophysical applications in mind and therefore should be quite useful for anyone using such data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://xarray.pydata.org/en/stable/_images/dataset-diagram.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;loading-and-plotting-data&#34;&gt;Loading and plotting data&lt;/h3&gt;

&lt;p&gt;So how does it work? Let&amp;rsquo;s get started.&lt;br /&gt;
First load the default packages that we are going to use later.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Necessary defaults
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import pandas as pd
import os, sys

# Xarray
import xarray as xr

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next let us load some spatial data. In my example it is a stack of annual &lt;strong&gt;Landsat&lt;/strong&gt; composite images created for the period 1984-2017. Each composite quantifies the Enhanced Vegetation Index (&lt;strong&gt;EVI&lt;/strong&gt;) for a given year. My test data is roughly 100mB big, but all code presented can easily be scaled up to hundreds of GB. &lt;br /&gt;
In the newest xarray version we can use the fantastic &lt;strong&gt;rasterio&lt;/strong&gt; python interface to load commonly-used spatial data such as GeoTiffs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;route = &amp;quot;test_2063.tif&amp;quot;
ds = xr.open_rasterio(route).rename({&#39;x&#39;: &#39;Longitude&#39;, &#39;y&#39;: &#39;Latitude&#39;, &#39;band&#39;: &#39;year&#39;})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Spatial-temporal satellite data comes with at least 3 dimensions. The spatial dimensions describing the rows and columns of the matrix as well as a third dimension usually associated with time (year, year-month, date, &amp;hellip;). xarray furthermore loads any spatial attributes (such as spatial extents, cell resolution or geographic projections) it can find and assigns them to the DataArray. &lt;br /&gt;
Here is how my data looks: (note the attributes!)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)&amp;gt;
[69682660 values with dtype=float32]
Coordinates:
  * year       (year) int64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ...
  * Latitude   (Latitude) float64 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 ...
  * Longitude  (Longitude) float64 -87.94 -87.94 -87.94 -87.94 -87.94 -87.94 ...
Attributes:
    transform:   (-87.9403950755027, 0.0002694945852365922, 0.0, 32.599951492...
    crs:         +init=epsg:4326
    res:         (0.0002694945852365922, 0.00026949458523998555)
    is_tiled:    0
    nodatavals:  (-3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38, -3.4e+38,...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are going to the data and try to create a spatial visualization. xarray has inbuilt plotting capabilities to visualize both temporal and spatial data using matplotlib. In my example I first alter the temporal dimension to have the correct time axis. I furthermore convert my xarray DataArray to a DataSet (&lt;a href=&#34;http://xarray.pydata.org/en/stable/data-structures.html&#34; target=&#34;_blank&#34;&gt;more here on the difference &lt;/a&gt;) and also filter the array to encorporate only positive values.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Create a series with years
dates = pd.date_range(&#39;1984-01-01&#39;, periods=34,freq = &amp;quot;A&amp;quot;).year
ds.coords[&amp;quot;year&amp;quot;] = dates # Reset dates
ds = ds.to_dataset(&amp;quot;EVI&amp;quot;) # Convert to dataset
# Create a subset to run calcution on and correct scale + filter
img = ds.pipe(lambda x: x * 0.0001)
img = img.where(img &amp;gt;= 0)  
# Select the last item of year and plot it
img.isel(year=33).EVI.plot(robust=True,cmap=&#39;viridis&#39;)
# Finally plot a test year
plt.title(&#39;Annual EVI in year %s&#39; % (&#39;2017&#39;) )
plt.ylabel(&#39;Latitude&#39;)
plt.xlabel(&#39;Longitude&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://martin-jung.github.io/img/posts/AnnualEVI.png&#34; alt=&#34;Annual EVI as quantified by Landsat&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;basic-and-applied-calculations&#34;&gt;Basic and applied calculations&lt;/h3&gt;

&lt;p&gt;Next let us conduct some spatial-temporal analyses. &lt;br /&gt;
The xarray created above can in principle easily be summarized as a whole or on the pixel level. Both can be easily achieved by grouping observations on the pixel level.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds2.EVI.groupby(&#39;year&#39;).std().plot()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://martin-jung.github.io/img/posts/StdEVI.png&#34; alt=&#34;Standard deviation as time series&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Equally if we want to quantify the standard deviation across all 34 years in the stack&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds2.EVI.groupby(&#39;Longitude&#39;,&#39;Latitude&#39;).std(&#39;year&#39;).plot()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://martin-jung.github.io/img/posts/StdEVI_spatial.png&#34; alt=&#34;Standard deviation per pixel&#34; /&gt;&lt;/p&gt;

&lt;p&gt;But what if want to apply a defined method over each pixel in given xarray?
This is again possible, but a little bit more complicated.
First we need to define a function that performs the requested analysis and returns a single value. In my example I want to calculate &lt;a href=&#34;https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient&#34; target=&#34;_blank&#34;&gt;Kendall&amp;rsquo;s rank correlation test&lt;/a&gt; for each time series over all pixels. Remember how we imported the &lt;strong&gt;scipy&lt;/strong&gt; stats package at the start of this post? To save time and harddrive space, we simply want to know for this simple example whether EVI is &amp;ldquo;significantly&amp;rdquo; ($p = 0.05$) increasing over the whole time series.
So let us build a function:
&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def k_cor(x,y, pthres = 0.05, direction = True):
    &amp;quot;&amp;quot;&amp;quot;
    Uses the scipy stats module to calculate a Kendall correlation test
    :x vector: Input pixel vector to run tests on
    :y vector: The date input vector
    :pthres: Significance of the underlying test
    :direction: output only direction as output (-1 &amp;amp; 1)
    &amp;quot;&amp;quot;&amp;quot;
    # Check NA values
    co = np.count_nonzero(~np.isnan(x))
    if co &amp;lt; 4: # If fewer than 4 observations return -9999
        return -9999
    # Run the kendalltau test
    tau, p_value = stats.kendalltau(x, y)

    # Criterium to return results in case of Significance
    if p_value &amp;lt; pthres:
        # Check direction
        if direction:
            if tau &amp;lt; 0:
                return -1
            elif tau &amp;gt; 0:
                return 1
        else:
            return tau
    else:
      return 0  

# The function we are going to use for applying our kendal test per pixel
def kendall_correlation(x,y,dim=&#39;year&#39;):
    # x = Pixel value, y = a vector containing the date, dim == dimension
    return xr.apply_ufunc(
        k_cor, x , y,
        input_core_dims=[[dim], [dim]],
        vectorize=True, # !Important!
        output_dtypes=[int]
        )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next we execute our new function and to do so we first need to create a new DataArray that contains the y variable (date in our example). Afterwards we can apply our function on the loaded xarray.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = xr.DataArray(np.arange(len(ds2[&#39;year&#39;]))+1, dims=&#39;year&#39;,
                 coords={&#39;year&#39;: ds2[&#39;year&#39;]})  
r = kendall_correlation(ds2, x,&#39;year&#39;)                 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This whole computation took us roughly 1min and 54 seconds on my Lenovo laptop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://martin-jung.github.io/img/posts/kcorEVI.png&#34; alt=&#34;Correlation test&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;further-speed-ups&#34;&gt;Further speed ups&lt;/h3&gt;

&lt;p&gt;In many cases an execution on a local computer hardly makes sense and is time inefficient. Particularly if your data is too large to fit into memory. This is usually the case with earth-observation data, which can easily become larger than 10GB++ .&lt;/p&gt;

&lt;p&gt;Luckily xarray supports parallel execution via the optional Dask &lt;a href=&#34;http://xarray.pydata.org/en/stable/dask.html&#34; target=&#34;_blank&#34;&gt;integration&lt;/a&gt;. So what is dask? Compared to standard numpy array calculations it supports lazy evaluation of any supplied code. This means that your code is only executed on the dataset as soon as you tell dask+xarray to do so (via the &lt;strong&gt;compute()&lt;/strong&gt; function ). Dask divides your array into many small pieces, so called chunks, each of which is presumed to be small enough to fit into memory. Chunking your data enables better parallelization which can easily be scaled up over multiple CPU cores or entire clusters of machines.
&lt;img src=&#34;http://martin-jung.github.io/img/posts/grid_search_schedule.gif&#34; alt=&#34;Dask distributed scheduler&#34; /&gt;
&lt;br /&gt;
So how do we enable dask for your computation ? This is actually quite simple.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import dask stuff
import dask.array as da
from dask.diagnostics import ProgressBar

# Then while loading in your data specify that you want your data to be loaded as chunks
# A more optimal chunk size for your data can really speed up computation
# So ensure that your dataset is correctly chuncked.
ds = xr.open_rasterio(route,chunks={&#39;band&#39;: 34, &#39;x&#39;: 1000, &#39;y&#39;: 1000}).rename({&#39;x&#39;: &#39;Longitude&#39;, &#39;y&#39;: &#39;Latitude&#39;, &#39;band&#39;: &#39;year&#39; })                         

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will notice that your DataArray has now become a dask-array&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;&amp;lt;xarray.DataArray (year: 34, Latitude: 1318, Longitude: 1555)&amp;gt;
dask.array&amp;lt;shape=(34, 1318, 1555), dtype=float32, chunksize=(34, 100, 100)&amp;gt;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next you have to enable dask in your apply function and compute the result&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def kendall_correlation(x,y,dim=&#39;year&#39;):
    return xr.apply_ufunc(
        mk_cor, x , y,
        input_core_dims=[[dim], [dim]],
        vectorize=True,
        dask=&#39;parallelized&#39;, # Note the addition !!!
        output_dtypes=[int]
        )

# Make a little ProgressBar
with ProgressBar():
    # Until &#39;compute&#39; is run, no computation is executed
    r = kendall_correlation(ds2, x,&#39;year&#39;).compute()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running my computation with dask decreased the whole processing time down to 45s !&lt;/p&gt;

&lt;p&gt;But there is more we can do to further speed up the computation. By enabling &lt;a href=&#34;http://numba.pydata.org/&#34; target=&#34;_blank&#34;&gt;numba&lt;/a&gt; as just-in-time (JIT) byte compiler for our kendall correlation function we can squeeze a couple more seconds out. Numba compiles your python function into a byte-compiled snip that is a lot faster to execute.
One can enable the JIT compiler as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numba import jit #  Speedup for python functions

@jit(nogil=True) # Enable JIT compiler
def k_cor(x,y, pthres = 0.05, direction = True):
  ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note all this was run on a rather small dataset. The more data you have and the more computational intensive your analysis becomes, the more do the steps above improve your execution time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;My system info&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linux Ubuntu - Codename bionic&lt;/li&gt;
&lt;li&gt;Python 3.6.5&lt;/li&gt;
&lt;li&gt;Numpy 1.13.3&lt;/li&gt;
&lt;li&gt;Xarray 0.10.2&lt;/li&gt;
&lt;li&gt;dask 0.16.0&lt;/li&gt;
&lt;li&gt;numba 0.34.0&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Full source code as gist&lt;/strong&gt;&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/Martin-Jung/d1946abc4310bc5a1cd50c53d1b029a7.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>The impact of economic recession on land changes in Greece</title>
      <link>http://martin-jung.github.io/project/greece/</link>
      <pubDate>Sun, 20 May 2018 20:24:48 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/project/greece/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Icarus</title>
      <link>http://martin-jung.github.io/project/icarus/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/project/icarus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New professional website</title>
      <link>http://martin-jung.github.io/post/2017-new-professional-website/</link>
      <pubDate>Fri, 16 Jun 2017 00:00:00 +0100</pubDate>
      
      <guid>http://martin-jung.github.io/post/2017-new-professional-website/</guid>
      <description>&lt;p&gt;Welcome to my new starting website. My name is Martin Jung and I consider myself an Environmental and data scientist.  &lt;a href=&#34;https://martin-jung.github.io/#about&#34; target=&#34;_blank&#34;&gt;Read here for more&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I have abandoned my previous online presence, the &lt;a href=&#34;http://conservationecology.wordpress.com/&#34; target=&#34;_blank&#34;&gt;Conservation ecology blog&lt;/a&gt; since quite a while. Mostly because I was too preoccupied working on my PhD. However I also disliked the inflexibility of wordpress layouts and the added advertising below each wordpress.com article. Therefore the restart of my professional website powered by hugo and the academic theme. So feel welcome if you are a previous reader of my blog, but notice that I will put more emphasis on examples and demonstrations on this website.&lt;/p&gt;

&lt;p&gt;Please do not hesitate to &lt;a href=&#34;https://martin-jung.github.io/#contact&#34; target=&#34;_blank&#34;&gt;contact me&lt;/a&gt;.&lt;br /&gt;
I&amp;rsquo;m particularly interested in scientific collaborations, consulting or data analyses offers. I also regularly give demonstrations and workshops on scientific data analysis (using R, python or QGIS), general data management (SQL database systems) and high performance computing. So please contact me if you think my services could be useful for your research group or company.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2017 Data analysis talk</title>
      <link>http://martin-jung.github.io/talk/2017_dacmtalk/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/talk/2017_dacmtalk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Birds in the matrix: the role of agriculture in avian conservation in the Taita Hills, Kenya</title>
      <link>http://martin-jung.github.io/publication/2017_birdsinthematrixafrica/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2017_birdsinthematrixafrica/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Remotely sensed forest monitoring in Bialowieza forest</title>
      <link>http://martin-jung.github.io/project/bialowieza/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/project/bialowieza/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Local factors mediate the response of biodiversity to land use on two African mountains</title>
      <link>http://martin-jung.github.io/publication/2017_mscthesisanimalconservation/</link>
      <pubDate>Wed, 28 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://martin-jung.github.io/publication/2017_mscthesisanimalconservation/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
